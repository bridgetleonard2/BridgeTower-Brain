{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import BridgeTowerModel, BridgeTowerProcessor, BridgeTowerImageProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Extraction\n",
    "Now that we've been introduced to BridgeTower and how it can be used to extract feature vectors from movies, let's apply this to the original dataset. The stimuli used in the original experiments were movie clips (https://gin.g-node.org/gallantlab/shortclips/src/master/stimuli) and transcripts of stories (https://openneuro.org/datasets/ds003020). We will do forward passess of each into our model to extract feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Get Movie Stimuli\n",
    "There are 12 training movies (0-11) and 1 test movie. We should end this first part with 13 datasets each containing 270-300 feature vectors with 768 dimensions each (8100-9000 images presented at 15 frames per second, average over every 2 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our movie stimuli is in hdf5 format such that each file in `stimuli` contains:\n",
    "    stimuli: array of shape (n_images, 512, 512, 3)\n",
    "        Each training run contains 9000 images total.\n",
    "        The test run contains 8100 images total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5_array(file_name, key=None, slice=slice(0, None)):\n",
    "    \"\"\"Function to load data from an hdf file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name: string\n",
    "        hdf5 file name.\n",
    "    key: string\n",
    "        Key name to load. If not provided, all keys will be loaded.\n",
    "    slice: slice, or tuple of slices\n",
    "        Load only a slice of the hdf5 array. It will load `array[slice]`.\n",
    "        Use a tuple of slices to get a slice in multiple dimensions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array or dictionary\n",
    "        Array, or dictionary of arrays (if `key` is None).\n",
    "    \"\"\"\n",
    "    with h5py.File(file_name, mode='r') as hf:\n",
    "        if key is None:\n",
    "            data = dict()\n",
    "            for k in hf.keys():\n",
    "                data[k] = hf[k][slice]\n",
    "            return data\n",
    "        else:\n",
    "            return hf[key][slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\test.hdf', key='stimuli')\n",
    "train_00 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_00.hdf', key='stimuli')\n",
    "train_01 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_01.hdf', key='stimuli')\n",
    "train_02 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_02.hdf', key='stimuli')\n",
    "train_03 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_03.hdf', key='stimuli')\n",
    "train_04 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_04.hdf', key='stimuli')\n",
    "train_05 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_05.hdf', key='stimuli')\n",
    "train_06 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_06.hdf', key='stimuli')\n",
    "train_07 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_07.hdf', key='stimuli')\n",
    "train_08 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_08.hdf', key='stimuli')\n",
    "train_09 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_09.hdf', key='stimuli')\n",
    "train_10 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_10.hdf', key='stimuli')\n",
    "train_11 = load_hdf5_array('bridgetower\\\\data\\\\shortclips\\\\stimuli\\\\train_11.hdf', key='stimuli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8100, 512, 512, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "plt.imshow(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Create functions\n",
    "Let's create some quick functions to streamline the process of extracting features from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "# model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BridgeTowerModel.from_pretrained(\"BridgeTower/bridgetower-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# placeholder for batch features\n",
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        # detached_outputs = [tensor.detach() for tensor in output]\n",
    "        last_output = output[-1].detach()\n",
    "        features[name] = last_output  # detached_outputs\n",
    "    return hook\n",
    "\n",
    "# register forward hooks with layers of choice\n",
    "# First, convolutional layers\n",
    "patch_embed = model.cross_modal_image_pooler.register_forward_hook(get_features('layer_8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[0, 2]]), 'attention_mask': tensor([[1, 1]]), 'pixel_values': tensor([[[[-1.2667, -1.1937, -1.2229,  ..., -1.4565, -1.4273, -1.4419],\n",
       "          [-1.2521, -1.2229, -0.9310,  ..., -0.9748, -1.3835, -1.5733],\n",
       "          [-1.2667, -1.2667, -0.6974,  ..., -1.1499, -1.4273, -1.5733],\n",
       "          ...,\n",
       "          [-1.5441, -1.5295, -1.5295,  ..., -1.5733, -1.5733, -1.6171],\n",
       "          [-1.5587, -1.5295, -1.5587,  ..., -1.5441, -1.5733, -1.6317],\n",
       "          [-1.5879, -1.5733, -1.5879,  ..., -1.4857, -1.5733, -1.6171]],\n",
       "\n",
       "         [[-0.9717, -0.9567, -0.9417,  ..., -0.9267, -0.9117, -0.9117],\n",
       "          [-0.9267, -0.9867, -0.6265,  ..., -0.3714, -0.8666, -1.0617],\n",
       "          [-0.9267, -1.0017, -0.3714,  ..., -0.6265, -0.9417, -1.0918],\n",
       "          ...,\n",
       "          [-1.4219, -1.4369, -1.4369,  ..., -1.1368, -1.0467, -1.0017],\n",
       "          [-1.4369, -1.4369, -1.4669,  ..., -1.1068, -1.0467, -1.0317],\n",
       "          [-1.4669, -1.4820, -1.5120,  ..., -1.0617, -1.0617, -1.0617]],\n",
       "\n",
       "         [[-1.1247, -1.0963, -1.1389,  ..., -1.0821, -1.0394, -1.1105],\n",
       "          [-1.0963, -1.1247, -0.7123,  ..., -0.5986, -0.9967, -1.2385],\n",
       "          [-1.0821, -1.1674, -0.4137,  ..., -0.7550, -1.0536, -1.2243],\n",
       "          ...,\n",
       "          [-0.7266, -0.7550, -0.8119,  ..., -1.1532, -1.2100, -1.2243],\n",
       "          [-0.7408, -0.7408, -0.8403,  ..., -1.1389, -1.2100, -1.2385],\n",
       "          [-0.7692, -0.7977, -0.8830,  ..., -1.1105, -1.2385, -1.2811]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(test[0], \"\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vecs(stim_data, n=30):\n",
    "    # create overall data structure for average feature vectors\n",
    "    # a dictionary with layer names as keys and a list of vectors as it values\n",
    "    data = {}\n",
    "\n",
    "    # a dictionary to store vectors for n consecutive trials\n",
    "    avg_data = {}\n",
    "\n",
    "    # loop through all inputs\n",
    "    for i, image in enumerate(stim_data):\n",
    "\n",
    "        model_input = processor(image, \"\", return_tensors=\"pt\")\n",
    "        \n",
    "        preds = model(**model_input)\n",
    "\n",
    "        for name, tensor in features.items():\n",
    "            if name not in avg_data:\n",
    "                avg_data[name] = []\n",
    "            avg_data[name].append(tensor)\n",
    "        \n",
    "        # check if average should be stored\n",
    "        if (i + 1) % n == 0:\n",
    "            for name, tensors in avg_data.items():\n",
    "                first_size = tensors[0].size()\n",
    "\n",
    "                if all(tensor.size() == first_size for tensor in tensors):\n",
    "                    avg_feature = torch.mean(torch.stack(tensors), dim=0)\n",
    "                else:\n",
    "                    # Find problem dimension\n",
    "                    for dim in range(tensors[0].dim()):\n",
    "                        first_dim = tensors[0].size(dim)\n",
    "\n",
    "                        if not all(tensor.size(dim) == first_dim for tensor in tensors):\n",
    "                            # Specify place to pad\n",
    "                            p_dim = (tensors[0].dim()*2) - (dim + 2)\n",
    "                            # print(p_dim)\n",
    "                            max_size = max(tensor.size(dim) for tensor in tensors)\n",
    "                            padded_tensors = []\n",
    "\n",
    "                            for tensor in tensors:\n",
    "                                pad_list = [0] * ((2*tensor[0].dim()) - 1 )  # Make a list with length of 2*dimensions - 1 to insert pad later\n",
    "                                pad_list.insert(p_dim, max_size - tensor.size(dim))\n",
    "                                # print(tuple(pad_list))\n",
    "                                padded_tensor = pad(tensor, tuple(pad_list))\n",
    "                                padded_tensors.append(padded_tensor)\n",
    "\n",
    "                    avg_feature = torch.mean(torch.stack(padded_tensors), dim=0)\n",
    "\n",
    "                if name not in data:\n",
    "                    data[name] = []\n",
    "                data[name].append(avg_feature)\n",
    "\n",
    "            avg_data = {}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_feature_vecs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
