{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Bridget Leonard\\\\Desktop\\\\BridgeTower-Brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Voxelwise Encoding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our stimuli features from the BridgeTower model, we can use them to predict brain responses. To do this, we will need to load in our fMRI data.\n",
    "To start we will be using sub1 from Popham et al 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_movie_train = np.load(\"data\\\\fmri_data\\\\moviedata\\\\S1\\\\train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_alternateithicatom = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\alternateithicatom.npy\")\n",
    "s1_avatar = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\avatar.npy\")\n",
    "s1_howtodraw = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\howtodraw.npy\")\n",
    "s1_legacy = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\legacy.npy\")\n",
    "s1_life = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\life.npy\")\n",
    "s1_myfirstdaywiththeyankees = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\myfirstdaywiththeyankees.npy\")\n",
    "s1_naked = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\naked.npy\")\n",
    "s1_odetostepfather = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\odetostepfather.npy\")\n",
    "s1_souls = np.load(\"bridgetower\\\\data\\\\storydata\\\\S1\\\\souls.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized as such:\n",
    "\n",
    "`moviedata`: Inside this folder, there are subfolders for each participant’s data as well as one folder for the stimuli\n",
    "\n",
    "Within each partcipant’s folder, there is a train.npy and a test.npy file, which contain the brain data for the training and testing data for the models. The shape of each of these is *[n_TR, z, x, y]*, and the last three dimensions should be flattened in order to match the shape of the vertex to voxel mappings in the mappers folder. Data that is located outside of the cortical mask is indicated by np.nan values.\n",
    "The shape of the data in the stimuli folder is [n_TR, n_features]. The features dimension includes the motion energy features (N=2139), which are then followed by the semantic features (N=985).\n",
    "\n",
    "`storydata`: Inside this folder, there are subfolders for each participant’s data as well as one folder for the stimuli\n",
    "\n",
    "Within each participant’s folder, there is a file for each stimulus which contains the brain data. The shape of each of these is *[n_TR, z, x, y]*, and the last three dimensions should be flattened in order to match the shape of the vertex to voxel mappings in the mappers folder. Data that is located outside of the cortical mask is indicated by np.nan values.\n",
    "The shape of the data in the stimuli folder is [n_TR, n_features]. The features dimension includes the low level features (number of words (N=1), number of phonemes (N=1), and phoneme counts (N=39)), which are then followed by the semantic features (N=985)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(s1_movie_train)\n",
    "\n",
    "# Apply the mask and then flatten\n",
    "# This will keep only the non-NaN values\n",
    "s1_movie_fmri = s1_movie_train[mask].reshape(s1_movie_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 81111)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_movie_fmri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2697605 ,  1.8778311 ,  1.896819  , -0.33944297, -0.24112298],\n",
       "       [ 1.1698035 ,  0.53661877,  0.19726549,  0.47479075, -0.02730744],\n",
       "       [ 0.8348491 ,  0.49176377, -0.8683965 , -0.53910434,  0.06158145],\n",
       "       [-1.3390145 , -0.02573461, -0.0707036 ,  0.11303766,  0.0237655 ],\n",
       "       [ 0.90123653,  0.7940609 ,  1.312021  ,  0.47119457, -0.03368266]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_movie_fmri[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelwise_normalization(fmri_data):\n",
    "    # Calculate mean and standard deviation for each voxel\n",
    "    mean_per_voxel = np.mean(fmri_data, axis=0)\n",
    "    std_per_voxel = np.std(fmri_data, axis=0)\n",
    "\n",
    "    # Perform voxel-wise normalization\n",
    "    normalized_fmri_data = (fmri_data - mean_per_voxel) / std_per_voxel\n",
    "\n",
    "    return normalized_fmri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_fmri = voxelwise_normalization(s1_movie_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3318814 ,  1.9632717 ,  1.9818691 , -0.37418306, -0.27609047],\n",
       "       [ 1.2262138 ,  0.5536382 ,  0.19786882,  0.47834775, -0.04839148],\n",
       "       [ 0.8721232 ,  0.50649494, -0.9207434 , -0.5832354 ,  0.04626913],\n",
       "       [-1.4259351 , -0.03740335, -0.08341502,  0.0995798 ,  0.00599771],\n",
       "       [ 0.94230336,  0.82421356,  1.3680139 ,  0.47458243, -0.05518066]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_fmri[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Load in feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie data\n",
    "train00 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_00_data.npy\")\n",
    "train01 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_01_data.npy\")\n",
    "train02 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_02_data.npy\")\n",
    "train03 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_03_data.npy\")\n",
    "train04 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_04_data.npy\")\n",
    "train05 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_05_data.npy\")\n",
    "train06 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_06_data.npy\")\n",
    "train07 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_07_data.npy\")\n",
    "train08 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_08_data.npy\")\n",
    "train09 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_09_data.npy\")\n",
    "train10 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_10_data.npy\")\n",
    "train11 = np.load(\"bridgetower\\\\feature_vectors\\\\movie\\\\train_11_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3870, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to combine all the training vectors so that they correspond to the fmri training dataset\n",
    "movie_features = np.vstack((train00, train01, train02, train03, train04, train05, train06, train07, train08, train09, train10, train11))\n",
    "movie_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story data\n",
    "alternateithicatom = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\alternateithicatom_data.npy\")\n",
    "avatar = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\avatar_data.npy\")\n",
    "howtodraw = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\howtodraw_data.npy\")\n",
    "legacy = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\legacy_data.npy\")\n",
    "life = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\life_data.npy\")\n",
    "myfirstdaywiththeyankees = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\myfirstdaywiththeyankees_data.npy\")\n",
    "naked = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\naked_data.npy\")\n",
    "odetostepfather = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\odetostepfather_data.npy\")\n",
    "souls = np.load(\"bridgetower\\\\feature_vectors\\\\story\\\\souls_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Voxelwise Encoding Model for Movie Data\n",
    "We want to create a voxelwise encoding model that captures the relationship between stimuli features from bridgetower and the fmri responses. \n",
    "\n",
    "Since our movie data was create with the same temporal alignment (TR = 2) as our fMRI data, we do not need to do any resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Finite impulse response model\n",
    "Understanding the Delays: fMRI responses to stimuli are not instantaneous but follow the hemodynamic response function (HRF), which peaks around 4-6 seconds after the stimulus. By selecting delays of 2, 4, 6, and 8 seconds, you're aiming to capture the rise, peak, and fall of this hemodynamic response relative to your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming features is your (3600, 768) feature data\n",
    "# And TR (time between scans) is 2 seconds\n",
    "delays = [2, 4, 6, 8]  # Delays in seconds\n",
    "shifted_features_list = []\n",
    "\n",
    "for delay in delays:\n",
    "    shift_amount = delay // 2  # Assuming TR is 2 seconds\n",
    "    shifted = np.roll(movie_features, shift_amount, axis=0)\n",
    "    # Optionally, handle edge effects here (e.g., zero-padding or trimming)\n",
    "    shifted_features_list.append(shifted)\n",
    "\n",
    "# Stack the shifted arrays to create a 3D array\n",
    "shifted_features_3d = np.stack(shifted_features_list, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using L2-regularized linear regression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 81111 vertices, we want to calculate it's relationship with the 4*768 delayed features. Where k=768 and m is our vertex of interest:\n",
    "\n",
    "`m = 4k * beta`\n",
    "\n",
    "So, after running regression, we should have a matrix of m # of rows and 4k # of columns, where each cell contains the beta values or linear weights on the 4k delayed features for each voxel.\n",
    "\n",
    "We are told in the dataset that data that is located outside of the cortical mask is indicated by np.nan values so we should ignore these in our regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'features_shifted' is your 3D array (time points x features x delays)\n",
    "# And 'fmri_data' is your 2D fMRI data (time points x voxels)\n",
    "\n",
    "# Reshape the feature data for regression\n",
    "n_time_points, n_features, n_delays = shifted_features_3d.shape\n",
    "features_reshaped = shifted_features_3d.reshape(n_time_points, n_features * n_delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3870, 3072)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_voxels = s1_movie_fmri.shape[1]\n",
    "alphas = np.logspace(-6, 6, 50) # Range for alphas\n",
    "kf = KFold(n_splits=5) # Example of 5-fold cross-validation\n",
    "\n",
    "best_alphas = np.zeros(n_voxels)\n",
    "coefficients = np.zeros((n_voxels, features_reshaped.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for voxel in range(n_voxels):\n",
    "    ridge_cv = RidgeCV(alphas=alphas, cv=kf)\n",
    "    ridge_cv.fit(features_reshaped, s1_movie_fmri[:, voxel])\n",
    "    best_alphas[voxel] = ridge_cv.alpha_\n",
    "    coefficients[voxel, :] = ridge_cv.coef_\n",
    "\n",
    "    #alphas = np.array(best_alphas)\n",
    "    #coef = np.array(coefficients)\n",
    "    np.save('bridgetower\\\\data\\\\encoding_models\\\\movie\\\\best_alphas.npy', best_alphas)\n",
    "    np.save('bridgetower\\\\data\\\\encoding_models\\\\movie\\\\coefficients.npy', coefficients)\n",
    "    # Print checkpoints\n",
    "    if voxel == round(n_voxels*0.1):\n",
    "        print(\"10% done\")\n",
    "    elif voxel == round(n_voxels*0.2):\n",
    "        print(\"20% done\")\n",
    "    elif voxel == round(n_voxels*0.3):\n",
    "        print(\"30% done\")\n",
    "    elif voxel == round(n_voxels*0.4):\n",
    "        print(\"40% done\")\n",
    "    elif voxel == round(n_voxels*0.5):\n",
    "        print(\"50% done\")\n",
    "    elif voxel == round(n_voxels*0.6):\n",
    "        print(\"60% done\")\n",
    "    elif voxel == round(n_voxels*0.7):\n",
    "        print(\"70% done\")\n",
    "    elif voxel == round(n_voxels*0.8):\n",
    "        print(\"80% done\")\n",
    "    elif voxel == round(n_voxels*0.9):\n",
    "        print(\"90% done\")\n",
    "print(\"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
