{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Full Protocol\n",
    "#### With GPU-accelerated Ridge Regression Using the Himalaya Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tutorial walks through the full crossmodal fMRI prediction process using the BridgeTower model. We will walk through extracting features from natural stimuli using BridgeTower layers, building voxelwise encoding models to predict fMRI data from stimuli features, and finally predicting language fMRI data using the vision encoding model and predicting visual fMRI data using the language encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters\n",
    "subject = 'S1'  # S1-S5\n",
    "modality = 'vision'  # vision or language\n",
    "layer = 8  # 1-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Feature Extraction\n",
    "We'll begin by putting our natural stimuli through the BridgeTower model and extracting feature representations from the layer specified above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Movie Stimuli\n",
    "Our movie data are stored in HDF format so we need a helper function to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5_array(file_name, key=None, slice=slice(0, None)):\n",
    "    \"\"\"Function to load data from an hdf file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name: string\n",
    "        hdf5 file name.\n",
    "    key: string\n",
    "        Key name to load. If not provided, all keys will be loaded.\n",
    "    slice: slice, or tuple of slices\n",
    "        Load only a slice of the hdf5 array. It will load `array[slice]`.\n",
    "        Use a tuple of slices to get a slice in multiple dimensions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array or dictionary\n",
    "        Array, or dictionary of arrays (if `key` is None).\n",
    "    \"\"\"\n",
    "    with h5py.File(file_name, mode='r') as hf:\n",
    "        if key is None:\n",
    "            data = dict()\n",
    "            for k in hf.keys():\n",
    "                data[k] = hf[k][slice]\n",
    "            return data\n",
    "        else:\n",
    "            return hf[key][slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/test.hdf', key='stimuli')\n",
    "train_00 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_00.hdf', key='stimuli')\n",
    "train_01 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_01.hdf', key='stimuli')\n",
    "train_02 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_02.hdf', key='stimuli')\n",
    "train_03 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_03.hdf', key='stimuli')\n",
    "train_04 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_04.hdf', key='stimuli')\n",
    "train_05 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_05.hdf', key='stimuli')\n",
    "train_06 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_06.hdf', key='stimuli')\n",
    "train_07 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_07.hdf', key='stimuli')\n",
    "train_08 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_08.hdf', key='stimuli')\n",
    "train_09 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_09.hdf', key='stimuli')\n",
    "train_10 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_10.hdf', key='stimuli')\n",
    "train_11 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_11.hdf', key='stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Story Stimuli\n",
    "Our story transcripts are in TextGrid format so we want to load them into a list, we'll create a helper function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textgrid_to_array(textgrid):\n",
    "    \"\"\"Function to load transcript from textgrid into a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    textgrid: string\n",
    "        TextGrid file name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_transcript : Array\n",
    "        Array with each word in the story.\n",
    "    \"\"\"\n",
    "    if textgrid == 'data/raw_stimuli/textgrids/stimuli/legacy.TextGrid':\n",
    "        with open(textgrid, 'r')as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        full_transcript = []\n",
    "        # Important info starts at line 5\n",
    "        for line in data[5:]:\n",
    "            if line.startswith('2'):\n",
    "                index = data.index(line)\n",
    "                word = re.search(r'\"([^\"]*)\"', data[index+1].strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "    elif textgrid == 'data/raw_stimuli/textgrids/stimuli/life.TextGrid':\n",
    "        with open(textgrid, 'r') as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        full_transcript = []\n",
    "        for line in data:\n",
    "            if \"word\" in line:\n",
    "                index = data.index(line)\n",
    "                words = data[index+6:]  # this is where first word starts\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i % 3 == 0:\n",
    "                word = re.search(r'\"([^\"]*)\"', word.strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "    else:\n",
    "        with open(textgrid, 'r') as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        # Important info starts at line 8\n",
    "        for line in data[8:]:\n",
    "            # We only want item [2] info because those are the words instead\n",
    "            # of phonemes\n",
    "            if \"item [2]\" in line:\n",
    "                index = data.index(line)\n",
    "\n",
    "        summary_info = [line.strip() for line in data[index+1:index+6]]\n",
    "        print(summary_info)\n",
    "\n",
    "        word_script = data[index+6:]\n",
    "        full_transcript = []\n",
    "        for line in word_script:\n",
    "            if \"intervals\" in line:\n",
    "                # keep track of which interval we're on\n",
    "                ind = word_script.index(line)\n",
    "                word = re.search(r'\"([^\"]*)\"',\n",
    "                                 word_script[ind+3].strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "\n",
    "    return np.array(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternateithicatom = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/alternateithicatom.TextGrid\")\n",
    "avatar = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/avatar.TextGrid\")\n",
    "howtodraw = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/howtodraw.TextGrid\")\n",
    "legacy = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/legacy.TextGrid\")\n",
    "life = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/life.TextGrid\")\n",
    "myfirstdaywiththeyankees = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/myfirstdaywiththeyankees.TextGrid\")\n",
    "naked = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/naked.TextGrid\")\n",
    "odetostepfather = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/odetostepfather.TextGrid\")\n",
    "souls = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/souls.TextGrid\")\n",
    "undertheinfluence = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/undertheinfluence.TextGrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run Stimuli through Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Helper Functions\n",
    "We need three functions, one to set-up the BridgeTower model, one to run the movie stimuli, and one to run the story stimuli\n",
    "\n",
    "**Note**: These functions use GPU acceleration through PyTorch. They will take significantly longer using CPUs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(layer):\n",
    "    \"\"\"Function to setup transformers model with layer hooks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer: int\n",
    "        A layer reference for the BridgeTower model. Set's the forward\n",
    "        hook on the relevant layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    device : cuda or cpu for gpu acceleration if accessible.\n",
    "    model: BridgeTower model.\n",
    "    processor: BridgeTower processor.\n",
    "    features: Dictionary\n",
    "        A placeholder for batch features, one for each forward\n",
    "        hook.\n",
    "    layer_selected: Relevant layer chosen for forward hook.\n",
    "    \"\"\"\n",
    "    # Define Model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BridgeTowerModel.from_pretrained(\"BridgeTower/bridgetower-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define layers\n",
    "    model_layers = {\n",
    "            1: model.cross_modal_text_transform,\n",
    "            2: model.cross_modal_image_transform,\n",
    "            3: model.token_type_embeddings,\n",
    "            4: model.vision_model.visual.ln_post,\n",
    "            5: model.text_model.encoder.layer[-1].output.LayerNorm,\n",
    "            6: model.cross_modal_image_layers[-1].output,\n",
    "            7: model.cross_modal_text_layers[-1].output,\n",
    "            8: model.cross_modal_image_pooler,\n",
    "            9: model.cross_modal_text_pooler,\n",
    "            10: model.cross_modal_text_layernorm,\n",
    "            11: model.cross_modal_image_layernorm,\n",
    "            12: model.cross_modal_text_link_tower[-1],\n",
    "            13: model.cross_modal_image_link_tower[-1],\n",
    "        }\n",
    "\n",
    "    # placeholder for batch features\n",
    "    features = {}\n",
    "\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            # detached_outputs = [tensor.detach() for tensor in output]\n",
    "            last_output = output[-1].detach()\n",
    "            features[name] = last_output  # detached_outputs\n",
    "        return hook\n",
    "\n",
    "    # register forward hooks with layers of choice\n",
    "    layer_selected = model_layers[layer].register_forward_hook(\n",
    "        get_features(f'layer_{layer}'))\n",
    "\n",
    "    processor = BridgeTowerProcessor.from_pretrained(\n",
    "        \"BridgeTower/bridgetower-base\")\n",
    "\n",
    "    return device, model, processor, features, layer_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_features(movie_data, layer, n=30):\n",
    "    \"\"\"Function to average feature vectors over every n inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_data: Array\n",
    "        An array of shape (n_images, 512, 512). Represents frames from\n",
    "        a color movie.\n",
    "    n (optional): int\n",
    "        Number of frames to average over. Set at 30 to mimick an MRI\n",
    "        TR = 2 with a 15 fps movie.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Dictionary\n",
    "        Dictionary where keys are the model layer from which activations are\n",
    "        extracted. Values are lists representing activations of 768 dimensions\n",
    "        over the course of n_images / 30.\n",
    "    \"\"\"\n",
    "    print(\"loading HDF array\")\n",
    "    movie_data = load_hdf5_array(movie_data, key='stimuli')\n",
    "\n",
    "    print(\"Running movie through model\")\n",
    "\n",
    "    # Define Model\n",
    "    device, model, processor, features, layer_selected = setup_model(layer)\n",
    "\n",
    "    # create overall data structure for average feature vectors\n",
    "    # a dictionary with layer names as keys and a list of vectors as it values\n",
    "    data = {}\n",
    "\n",
    "    # a dictionary to store vectors for n consecutive trials\n",
    "    avg_data = {}\n",
    "\n",
    "    # loop through all inputs\n",
    "    for i, image in enumerate(movie_data):\n",
    "\n",
    "        model_input = processor(image, \"\", return_tensors=\"pt\")\n",
    "        # Assuming model_input is a dictionary of tensors\n",
    "        model_input = {key: value.to(device) for key,\n",
    "                       value in model_input.items()}\n",
    "\n",
    "        _ = model(**model_input)\n",
    "\n",
    "        for name, tensor in features.items():\n",
    "            if name not in avg_data:\n",
    "                avg_data[name] = []\n",
    "            avg_data[name].append(tensor)\n",
    "\n",
    "        # check if average should be stored\n",
    "        if (i + 1) % n == 0:\n",
    "            for name, tensors in avg_data.items():\n",
    "                first_size = tensors[0].size()\n",
    "\n",
    "                if all(tensor.size() == first_size for tensor in tensors):\n",
    "                    avg_feature = torch.mean(torch.stack(tensors), dim=0)\n",
    "                else:\n",
    "                    # Find problem dimension\n",
    "                    for dim in range(tensors[0].dim()):\n",
    "                        first_dim = tensors[0].size(dim)\n",
    "\n",
    "                        if not all(tensor.size(dim) == first_dim\n",
    "                                   for tensor in tensors):\n",
    "                            # Specify place to pad\n",
    "                            p_dim = (tensors[0].dim()*2) - (dim + 2)\n",
    "                            # print(p_dim)\n",
    "                            max_size = max(tensor.size(dim)\n",
    "                                           for tensor in tensors)\n",
    "                            padded_tensors = []\n",
    "\n",
    "                            for tensor in tensors:\n",
    "                                # Make a list with length of 2*dimensions - 1\n",
    "                                # to insert pad later\n",
    "                                pad_list = [0] * ((2*tensor[0].dim()) - 1)\n",
    "                                pad_list.insert(\n",
    "                                    p_dim, max_size - tensor.size(dim))\n",
    "                                # print(tuple(pad_list))\n",
    "                                padded_tensor = pad(tensor, tuple(pad_list))\n",
    "                                padded_tensors.append(padded_tensor)\n",
    "\n",
    "                    avg_feature = torch.mean(torch.stack(padded_tensors),\n",
    "                                             dim=0)\n",
    "\n",
    "                if name not in data:\n",
    "                    data[name] = []\n",
    "                data[name].append(avg_feature)\n",
    "\n",
    "            avg_data = {}\n",
    "\n",
    "    layer_selected.remove()\n",
    "\n",
    "    # Save data\n",
    "    data = data[f'layer_{layer}'].cpu()\n",
    "    data = data.numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_features(story_data, layer, n=20):\n",
    "    \"\"\"Function to extract feature vectors for each word of a story.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    story_data: Array\n",
    "        An array containing each word of the story in order.\n",
    "    n (optional): int\n",
    "        Number of words to to pad the target word with for\n",
    "        context (before and after).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Dictionary\n",
    "        Dictionary where keys are the model layer from which activations are\n",
    "        extracted. Values are lists representing activations of 768 dimensions\n",
    "        over the course of each word in the story.\n",
    "    \"\"\"\n",
    "    print(\"loading textgrid\")\n",
    "    story_data = textgrid_to_array(story_data)\n",
    "\n",
    "    print(\"Running story through model\")\n",
    "    # Define Model\n",
    "    device, model, processor, features, layer_selected = setup_model(layer)\n",
    "\n",
    "    # Create a numpy array filled with gray values (128 in this case)\n",
    "    # THis will act as tthe zero image input***\n",
    "    gray_value = 128\n",
    "    image_array = np.full((512, 512, 3), gray_value, dtype=np.uint8)\n",
    "\n",
    "    # create overall data structure for average feature vectors\n",
    "    # a dictionary with layer names as keys and a list of vectors as it values\n",
    "    data = {}\n",
    "\n",
    "    # loop through all inputs\n",
    "    for i, word in enumerate(story_data):\n",
    "        # if one of first 20 words, just pad with all the words before it\n",
    "        if i < n:\n",
    "            # collapse list of strings into a single one\n",
    "            word_with_context = ' '.join(story_data[:(i+n)])\n",
    "        # if one of last 20 words, just pad with all the words after it\n",
    "        elif i > (len(story_data) - n):\n",
    "            # collapse list of strings into a single one\n",
    "            word_with_context = ' '.join(story_data[(i-n):])\n",
    "            # collapse list of strings into a single one\n",
    "        else:\n",
    "            word_with_context = ' '.join(story_data[(i-n):(i+n)])\n",
    "\n",
    "        model_input = processor(image_array, word_with_context,\n",
    "                                return_tensors=\"pt\")\n",
    "        # Assuming model_input is a dictionary of tensors\n",
    "        model_input = {key: value.to(device) for key,\n",
    "                       value in model_input.items()}\n",
    "\n",
    "        _ = model(**model_input)\n",
    "\n",
    "        for name, tensor in features.items():\n",
    "            if name not in data:\n",
    "                data[name] = []\n",
    "            data[name].append(tensor)\n",
    "\n",
    "    layer_selected.remove()\n",
    "\n",
    "    # Save data\n",
    "    data = data[f'layer_{layer}'].cpu()\n",
    "    data = data.numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Run Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = get_movie_features(test, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train01_features = get_movie_features(train_01, layer)\n",
    "train02_features = get_movie_features(train_02, layer)\n",
    "train03_features = get_movie_features(train_03, layer)\n",
    "train04_features = get_movie_features(train_04, layer)\n",
    "train05_features = get_movie_features(train_05, layer)\n",
    "train06_features = get_movie_features(train_06, layer)\n",
    "train07_features = get_movie_features(train_07, layer)\n",
    "train08_features = get_movie_features(train_08, layer)\n",
    "train09_features = get_movie_features(train_09, layer)\n",
    "train10_features = get_movie_features(train_10, layer)\n",
    "train11_features = get_movie_features(train_11, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_features = get_story_features(alternateithicatom, layer)\n",
    "avatar_features = get_story_features(avatar, layer)\n",
    "howtodraw_features = get_story_features(howtodraw, layer)\n",
    "legacy_features = get_story_features(legacy, layer)\n",
    "life_features = get_story_features(life, layer)\n",
    "yankees_features = get_story_features(myfirstdaywiththeyankees, layer)\n",
    "naked_features = get_story_features(naked, layer)\n",
    "ode_features = get_story_features(odetostepfather, layer)\n",
    "souls_features = get_story_features(souls, layer)\n",
    "under_features = get_story_features(undertheinfluence, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Voxelwise Encoding Models\n",
    "Now that we have our stimuli features we can build linear models to map the relationship to the fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vision Encoding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fmri = np.load(\"data/fmri_data/moviedata/\" + subject + \"/train.npy\")\n",
    "test_fmri = np.load(\"data/fmri_data/moviedata/\" + subject + \"/test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Prep the data\n",
    "We need a few helper functions to help us prep the data. One is to remove NaNs from the fMRI data (those are out of range values) and one is to generate the leave one out protocol for the Ridge Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(data):\n",
    "    mask = ~np.isnan(data)\n",
    "\n",
    "    # Apply the mask and then flatten\n",
    "    # This will keep only the non-NaN values\n",
    "    data_reshaped = data[mask].reshape(data.shape[0], -1)\n",
    "\n",
    "    print(\"fMRI shape:\", data_reshaped.shape)\n",
    "    return data_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_leave_one_run_out(n_samples, run_onsets, random_state=None,\n",
    "                               n_runs_out=1):\n",
    "    \"\"\"Generate a leave-one-run-out split for cross-validation.\n",
    "\n",
    "    Generates as many splits as there are runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Total number of samples in the training set.\n",
    "    run_onsets : array of int of shape (n_runs, )\n",
    "        Indices of the run onsets.\n",
    "    random_state : None | int | instance of RandomState\n",
    "        Random state for the shuffling operation.\n",
    "    n_runs_out : int\n",
    "        Number of runs to leave out in the validation set. Default to one.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    train : array of int of shape (n_samples_train, )\n",
    "        Training set indices.\n",
    "    val : array of int of shape (n_samples_val, )\n",
    "        Validation set indices.\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    n_runs = len(run_onsets)\n",
    "    # With permutations, we are sure that all runs are used as validation runs.\n",
    "    # However here for n_runs_out > 1, a run can be chosen twice as validation\n",
    "    # in the same split.\n",
    "    all_val_runs = np.array(\n",
    "        [random_state.permutation(n_runs) for _ in range(n_runs_out)])\n",
    "\n",
    "    all_samples = np.arange(n_samples)\n",
    "    runs = np.split(all_samples, run_onsets[1:])\n",
    "    if any(len(run) == 0 for run in runs):\n",
    "        raise ValueError(\"Some runs have no samples. Check that run_onsets \"\n",
    "                         \"does not include any repeated index, nor the last \"\n",
    "                         \"index.\")\n",
    "\n",
    "    for val_runs in all_val_runs.T:\n",
    "        train = np.hstack(\n",
    "            [runs[jj] for jj in range(n_runs) if jj not in val_runs])\n",
    "        val = np.hstack([runs[jj] for jj in range(n_runs) if jj in val_runs])\n",
    "        yield train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_fmri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove nans from fMRI data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_fmri_clean \u001b[38;5;241m=\u001b[39m remove_nan(\u001b[43mtrain_fmri\u001b[49m)\n\u001b[0;32m      3\u001b[0m test_fmri_clean \u001b[38;5;241m=\u001b[39m remove_nan(test_fmri)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_fmri' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove nans from fMRI data\n",
    "train_fmri_clean = remove_nan(train_fmri)\n",
    "test_fmri_clean = remove_nan(test_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify feature data and fMRI data\n",
    "vision_fmri_arrays = [train_fmri_clean, test_fmri_clean]\n",
    "vision_feature_arrays = [train00_features, train01_features, train02_features,\n",
    "                  train03_features, train04_features, train05_features,\n",
    "                  train06_features, train07_features, train08_features,\n",
    "                  train09_features, train10_features, train11_features,\n",
    "                  test_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're creating voxelwise encoding models that will be used to predict fMRI data for the opposite modality, we don't need to hold out any data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "vision_Y_train = np.vstack(vision_fmri_arrays)\n",
    "vision_X_train = np.vstack(vision_feature_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Set-Up Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also don't want to split a single feature in half during cross validation so we will split them based on their onsets in the X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_onsets(feature_arrays):\n",
    "    \"\"\"Function to get run onsets from feature arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_arrays: List\n",
    "        List of feature arrays.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    run_onsets : Array\n",
    "        Array of run onsets.\n",
    "    \"\"\"\n",
    "    run_onsets = [0]\n",
    "    for array in feature_arrays:\n",
    "        run_onsets.append(run_onsets[-1] + array.shape[0])\n",
    "\n",
    "    return run_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_arrays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m run_onsets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m current_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfeature_arrays\u001b[49m:\n\u001b[0;32m      5\u001b[0m     next_index \u001b[38;5;241m=\u001b[39m current_index \u001b[38;5;241m+\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m     run_onsets\u001b[38;5;241m.\u001b[39mappend(current_index)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_arrays' is not defined"
     ]
    }
   ],
   "source": [
    "# Define cross-validation\n",
    "vision_run_onsets = run_onsets(vision_feature_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ridge(train_data, run_onsets):\n",
    "    n_samples_train = train_data.shape[0]\n",
    "    cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
    "    cv = check_cv(cv)  # cross-validation splitter into a reusable list\n",
    "    # Define the model\n",
    "    scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "    delayer = Delayer(delays=[1, 2, 3, 4])\n",
    "\n",
    "    backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "    print(backend)\n",
    "\n",
    "    vision_X_train = vision_X_train.astype(\"float32\")\n",
    "\n",
    "    alphas = np.logspace(1, 20, 20)\n",
    "\n",
    "    ridge_cv = RidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                            n_targets_batch_refit=100))\n",
    "\n",
    "    pipeline = make_pipeline(\n",
    "        scaler,\n",
    "        delayer,\n",
    "        ridge_cv,\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pipeline = setup_ridge(vision_X_train, vision_run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Run Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "_ = vision_pipeline.fit(vision_X_train, vision_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients\n",
    "vision_coef = vision_pipeline[-1].coef_\n",
    "vision_coef = backend.to_numpy(vision_coef)\n",
    "print(\"(n_delays * n_features, n_voxels) =\", vision_coef.shape)\n",
    "\n",
    "# Regularize coefficients\n",
    "vision_coef /= np.linalg.norm(vision_coef, axis=0)[None]\n",
    "# coef *= np.sqrt(np.maximum(0, scores))[None]\n",
    "\n",
    "# split the ridge coefficients per delays\n",
    "delayer = vision_pipeline.named_steps['delayer']\n",
    "vision_coef_per_delay = delayer.reshape_by_delays(vision_coef, axis=0)\n",
    "print(\"(n_delays, n_features, n_voxels) =\", vision_coef_per_delay.shape)\n",
    "del vision_coef\n",
    "\n",
    "# average over delays\n",
    "vision_average_coef = np.mean(vision_coef_per_delay, axis=0)\n",
    "print(\"(n_features, n_voxels) =\", vision_average_coef.shape)\n",
    "del vision_coef_per_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Language Encoding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_fmri = np.load(\"data/fmri_data/storydata\" + subject + \"/alternateithicatom.npy\")\n",
    "avatar_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/avatar.npy\")\n",
    "howtodraw_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/howtodraw.npy\")\n",
    "legacy_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/legacy.npy\")\n",
    "life_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/life.npy\")\n",
    "yankees_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/myfirstdaywiththeyankees.npy\")\n",
    "naked_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/naked.npy\")\n",
    "ode_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/odetostepfather.npy\")\n",
    "souls_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/souls.npy\")\n",
    "under_fmri = np.load(\"data/fmri_data/storydata/\" + subject + \"/undertheinfluence.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Prep the data\n",
    "We need a few additional helper functions to prep this data. A resampler function will help us resample the feature data to fMRI acquisition time (the movie features were already set to this). And a prep_data function will remove NaNs and do resampling together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_acq(feature_data, fmri_data):\n",
    "    dimensions = fmri_data.shape[0]\n",
    "    data_transposed = feature_data.T\n",
    "    data_resampled = np.empty((data_transposed.shape[0], dimensions))\n",
    "\n",
    "    for i in range(data_transposed.shape[0]):\n",
    "        data_resampled[i, :] = resample(data_transposed[i, :],\n",
    "                                        dimensions, window=('kaiser', 14))\n",
    "\n",
    "    print(\"Shape after resampling:\", data_resampled.T.shape)\n",
    "    return data_resampled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(fmri_data, feature_data):\n",
    "    fmri_reshaped = remove_nan(fmri_data)\n",
    "\n",
    "    feature_resampled = resample_to_acq(feature_data, fmri_reshaped)\n",
    "\n",
    "    return fmri_reshaped, feature_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample feature data and remove nans from fmri data\n",
    "ai_fmri_clean, ai_feat_resamp = prep_data(ai_fmri, ai_features)\n",
    "avatar_fmri, avatar_feat_resamp = prep_data(avatar_fmri, avatar_features)\n",
    "howtodraw_fmri_clean, howtodraw_feat_resamp = prep_data(howtodraw_fmri, howtodraw_features)\n",
    "legacy_fmri_clean, legacy_feat_resamp = prep_data(legacy_fmri, legacy_features)\n",
    "life_fmri_clean, life_feat_resamp = prep_data(life_fmri, life_features)\n",
    "yankees_fmri_clean, yankees_feat_resamp = prep_data(yankees_fmri, yankees_features)\n",
    "naked_fmri_clean, naked_feat_resamp = prep_data(naked_fmri, naked_features)\n",
    "ode_fmri_clean, ode_feat_resamp = prep_data(ode_fmri, ode_features)\n",
    "souls_fmri_clean, souls_feat_resamp = prep_data(souls_fmri, souls_features)\n",
    "under_fmri_clean, under_feat_resamp = prep_data(under_fmri, under_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcify feature data and fMRI data\n",
    "language_fmri_arrays = [ai_fmri_clean, avatar_fmri, howtodraw_fmri_clean,\n",
    "                        legacy_fmri_clean, life_fmri_clean, yankees_fmri_clean,\n",
    "                        naked_fmri_clean, ode_fmri_clean, souls_fmri_clean,\n",
    "                        under_fmri_clean]\n",
    "language_feature_arrays = [ai_feat_resamp, avatar_feat_resamp, howtodraw_feat_resamp,\n",
    "                           legacy_feat_resamp, life_feat_resamp, yankees_feat_resamp,\n",
    "                           naked_feat_resamp, ode_feat_resamp, souls_feat_resamp,\n",
    "                           under_feat_resamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since we're creating voxelwise encoding models that will be used to predict fMRI data for the opposite modality, we don't need to hold out any data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "language_Y_train = np.vstack(language_fmri_arrays)\n",
    "language_X_train = np.vstack(language_feature_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Set up Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation\n",
    "language_run_onsets = run_onsets(language_feature_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pipeline = setup_ridge(language_X_train, language_run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Run Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "_ = language_pipeline.fit(language_X_train, language_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients\n",
    "language_coef = language_pipeline[-1].coef_\n",
    "language_coef = backend.to_numpy(language_coef)\n",
    "print(\"(n_delays * n_features, n_voxels) =\", language_coef.shape)\n",
    "\n",
    "# Regularize coefficients\n",
    "language_coef /= np.linalg.norm(language_coef, axis=0)[None]\n",
    "# coef *= np.sqrt(np.maximum(0, scores))[None]\n",
    "\n",
    "# split the ridge coefficients per delays\n",
    "delayer = language_pipeline.named_steps['delayer']\n",
    "language_coef_per_delay = delayer.reshape_by_delays(language_coef, axis=0)\n",
    "print(\"(n_delays, n_features, n_voxels) =\", language_coef_per_delay.shape)\n",
    "del language_coef\n",
    "\n",
    "# average over delays\n",
    "language_average_coef = np.mean(language_coef_per_delay, axis=0)\n",
    "print(\"(n_features, n_voxels) =\", language_average_coef.shape)\n",
    "del language_coef_per_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Crossmodal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Project features into alternate modality\n",
    "Before we predict fMRI data using BridgeTower features we want to transform our features into the crossmodal space. To do this, we will use the Flickr dataset consisting of caption-image pairs to build two Ridge Regression models. One that maps BridgeTower visual features from images to BridgeTower language features from captions, and one that maps caption features to image features. This way we will have a image to caption matrix to project visual features into the language space and a caption to image matrix to project language features into the visual space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Extract Flickr features from BridgeTower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "flickr_test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "device, model, processor, features, layer_selected = setup_model(layer)\n",
    "\n",
    "flickr_features = []\n",
    "\n",
    "for item in range(len(flickr_test)):\n",
    "    image = flickr_test[item]['image']\n",
    "    image_array = np.array(image)\n",
    "    caption = \" \".join(flickr_test[item]['caption'])\n",
    "\n",
    "    # Run image\n",
    "    image_input = processor(image_array, \"\", return_tensors=\"pt\")\n",
    "    image_input = {key: value.to(device) for key,\n",
    "                    value in image_input.items()}\n",
    "\n",
    "    _ = model(**image_input)\n",
    "\n",
    "    image_vector = features[f'layer_{layer}']\n",
    "\n",
    "    # Run caption\n",
    "    # Create a numpy array filled with gray values (128 in this case)\n",
    "    # THis will act as tthe zero image input***\n",
    "    gray_value = 128\n",
    "    gray_image_array = np.full((512, 512, 3), gray_value, dtype=np.uint8)\n",
    "\n",
    "    caption_input = processor(gray_image_array, caption,\n",
    "                                return_tensors=\"pt\")\n",
    "    caption_input = {key: value.to(device) for key,\n",
    "                        value in caption_input.items()}\n",
    "    _ = model(**caption_input)\n",
    "\n",
    "    caption_vector = features[f'layer_{layer}']\n",
    "\n",
    "    flickr_features.append([image_vector, caption_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run encoding model\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "print(backend)\n",
    "\n",
    "# Variables\n",
    "captions = data[:, 1, :]\n",
    "images = data[:, 0, :]\n",
    "\n",
    "alphas = np.logspace(1, 20, 20)\n",
    "scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "ridge_cv = RidgeCV(\n",
    "    alphas=alphas, cv=5,\n",
    "    solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                        n_targets_batch_refit=100))\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    scaler,\n",
    "    ridge_cv\n",
    ")\n",
    "\n",
    "_ = pipeline.fit(images, captions)\n",
    "coef_images_to_captions = backend.to_numpy(pipeline[-1].coef_)\n",
    "\n",
    "_ = pipeline.fit(captions, images)\n",
    "coef_captions_to_images = backend.to_numpy(pipeline[-1].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Project movie features to language space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed = np.dot(test_features, coef_images_to_captions.T)\n",
    "train00_transformed = np.dot(train00_features, coef_images_to_captions.T)\n",
    "train01_transformed = np.dot(train01_features, coef_images_to_captions.T)\n",
    "train02_transformed = np.dot(train02_features, coef_images_to_captions.T)\n",
    "train03_transformed = np.dot(train03_features, coef_images_to_captions.T)\n",
    "train04_transformed = np.dot(train04_features, coef_images_to_captions.T)\n",
    "train05_transformed = np.dot(train05_features, coef_images_to_captions.T)\n",
    "train06_transformed = np.dot(train06_features, coef_images_to_captions.T)\n",
    "train07_transformed = np.dot(train07_features, coef_images_to_captions.T)\n",
    "train08_transformed = np.dot(train08_features, coef_images_to_captions.T)\n",
    "train09_transformed = np.dot(train09_features, coef_images_to_captions.T)\n",
    "train10_transformed = np.dot(train10_features, coef_images_to_captions.T)\n",
    "train11_transformed = np.dot(train11_features, coef_images_to_captions.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatmap(subject, layer, correlations, modality):\n",
    "    \"\"\"Function to run the vision encoding model. Predicts brain activity\n",
    "    to story listening and return correlations between predictions and real\n",
    "    brain activity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject: string\n",
    "        A reference to the subject for analysis. Used to load fmri data.\n",
    "    layer: int\n",
    "        A layer reference for the BridgeTower model. Set's the forward\n",
    "        hook on the relevant layer.\n",
    "    correlations: array\n",
    "        Generated by story_prediction() or movie_prediction() function.\n",
    "        Contains the correlation between predicted and real brain activity\n",
    "        for each voxel.\n",
    "    modality: string\n",
    "        Which modality was used for the base encoding model: vision or\n",
    "        language.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Flatmaps:\n",
    "        Saves flatmap visualizations as pngs\n",
    "    \"\"\"\n",
    "    # Reverse flattening and masking\n",
    "    fmri_alternateithicatom = np.load(\"data/storydata/\" + subject +\n",
    "                                      \"/alternateithicatom.npy\")\n",
    "\n",
    "    mask = ~np.isnan(fmri_alternateithicatom[0])  # reference for the mask\n",
    "    # Initialize an empty 3D array with NaNs for the correlation data\n",
    "    reconstructed_correlations = np.full((31, 100, 100), np.nan)\n",
    "\n",
    "    # Flatten the mask to get the indices of the non-NaN data points\n",
    "    valid_indices = np.where(mask.flatten())[0]\n",
    "\n",
    "    # Assign the correlation coefficients to their original spatial positions\n",
    "    for index, corr_value in zip(valid_indices, correlations):\n",
    "        # Convert the 1D index back to 3D index in the spatial dimensions\n",
    "        z, x, y = np.unravel_index(index, (31, 100, 100))\n",
    "        reconstructed_correlations[z, x, y] = corr_value\n",
    "\n",
    "    flattened_correlations = reconstructed_correlations.flatten()\n",
    "\n",
    "    # Load mappers\n",
    "    lh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_lh.npz\")\n",
    "    lh_vertex_correlation_data = lh_mapping_matrix.dot(flattened_correlations)\n",
    "    lh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_lh.npy\")\n",
    "\n",
    "    rh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_rh.npz\")\n",
    "    rh_vertex_correlation_data = rh_mapping_matrix.dot(flattened_correlations)\n",
    "    rh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_rh.npy\")\n",
    "\n",
    "    vmin, vmax = -0.1, 0.1\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4))\n",
    "\n",
    "    # Plot the first flatmap\n",
    "    sc1 = axs[0].scatter(lh_vertex_coords[:, 0], lh_vertex_coords[:, 1],\n",
    "                         c=lh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                         vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[0].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[0].set_title('Left Hemisphere')\n",
    "    axs[0].set_frame_on(False)\n",
    "    axs[0].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[0].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Plot the second flatmap\n",
    "    _ = axs[1].scatter(rh_vertex_coords[:, 0], rh_vertex_coords[:, 1],\n",
    "                       c=rh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                       vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[1].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[1].set_title('Right Hemisphere')\n",
    "    axs[1].set_frame_on(False)\n",
    "    axs[1].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[1].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Adjust layout to make space for the top colorbar\n",
    "    plt.subplots_adjust(top=0.85, wspace=0)\n",
    "\n",
    "    # Add a single horizontal colorbar at the top\n",
    "    cbar_ax = fig.add_axes([0.25, 0.9, 0.5, 0.03])\n",
    "    cbar = fig.colorbar(sc1, cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "    # Set the color bar to only display min and max values\n",
    "    cbar.set_ticks([vmin, vmax])\n",
    "    cbar.set_ticklabels([f'{vmin}', f'{vmax}'])\n",
    "\n",
    "    # Remove the color bar box\n",
    "    cbar.outline.set_visible(False)\n",
    "    if modality == 'vision':\n",
    "        latex = r\"$r_{\\mathit{movie \\rightarrow story}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "\n",
    "        plt.savefig('results/movie_to_story/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    elif modality == 'language':\n",
    "        latex = r\"$r_{\\mathit{story \\rightarrow movie}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "        plt.savefig('results/story_to_movie/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
