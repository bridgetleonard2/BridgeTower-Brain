{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Full Protocol\n",
    "#### With GPU-accelerated Ridge Regression Using the Himalaya Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tutorial walks through the full crossmodal fMRI prediction process using the BridgeTower model. We will walk through extracting features from natural stimuli using BridgeTower layers, building voxelwise encoding models to predict fMRI data from stimuli features, and finally predicting language fMRI data using the vision encoding model and predicting visual fMRI data using the language encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters\n",
    "subject = 'S1'  # S1-S5\n",
    "modality = 'vision'  # vision or language\n",
    "layer = 8  # 1-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatmap(subject, layer, correlations, modality):\n",
    "    \"\"\"Function to run the vision encoding model. Predicts brain activity\n",
    "    to story listening and return correlations between predictions and real\n",
    "    brain activity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject: string\n",
    "        A reference to the subject for analysis. Used to load fmri data.\n",
    "    layer: int\n",
    "        A layer reference for the BridgeTower model. Set's the forward\n",
    "        hook on the relevant layer.\n",
    "    correlations: array\n",
    "        Generated by story_prediction() or movie_prediction() function.\n",
    "        Contains the correlation between predicted and real brain activity\n",
    "        for each voxel.\n",
    "    modality: string\n",
    "        Which modality was used for the base encoding model: vision or\n",
    "        language.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Flatmaps:\n",
    "        Saves flatmap visualizations as pngs\n",
    "    \"\"\"\n",
    "    # Reverse flattening and masking\n",
    "    fmri_alternateithicatom = np.load(\"data/storydata/\" + subject +\n",
    "                                      \"/alternateithicatom.npy\")\n",
    "\n",
    "    mask = ~np.isnan(fmri_alternateithicatom[0])  # reference for the mask\n",
    "    # Initialize an empty 3D array with NaNs for the correlation data\n",
    "    reconstructed_correlations = np.full((31, 100, 100), np.nan)\n",
    "\n",
    "    # Flatten the mask to get the indices of the non-NaN data points\n",
    "    valid_indices = np.where(mask.flatten())[0]\n",
    "\n",
    "    # Assign the correlation coefficients to their original spatial positions\n",
    "    for index, corr_value in zip(valid_indices, correlations):\n",
    "        # Convert the 1D index back to 3D index in the spatial dimensions\n",
    "        z, x, y = np.unravel_index(index, (31, 100, 100))\n",
    "        reconstructed_correlations[z, x, y] = corr_value\n",
    "\n",
    "    flattened_correlations = reconstructed_correlations.flatten()\n",
    "\n",
    "    # Load mappers\n",
    "    lh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_lh.npz\")\n",
    "    lh_vertex_correlation_data = lh_mapping_matrix.dot(flattened_correlations)\n",
    "    lh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_lh.npy\")\n",
    "\n",
    "    rh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_rh.npz\")\n",
    "    rh_vertex_correlation_data = rh_mapping_matrix.dot(flattened_correlations)\n",
    "    rh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_rh.npy\")\n",
    "\n",
    "    vmin, vmax = -0.1, 0.1\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4))\n",
    "\n",
    "    # Plot the first flatmap\n",
    "    sc1 = axs[0].scatter(lh_vertex_coords[:, 0], lh_vertex_coords[:, 1],\n",
    "                         c=lh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                         vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[0].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[0].set_title('Left Hemisphere')\n",
    "    axs[0].set_frame_on(False)\n",
    "    axs[0].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[0].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Plot the second flatmap\n",
    "    _ = axs[1].scatter(rh_vertex_coords[:, 0], rh_vertex_coords[:, 1],\n",
    "                       c=rh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                       vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[1].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[1].set_title('Right Hemisphere')\n",
    "    axs[1].set_frame_on(False)\n",
    "    axs[1].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[1].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Adjust layout to make space for the top colorbar\n",
    "    plt.subplots_adjust(top=0.85, wspace=0)\n",
    "\n",
    "    # Add a single horizontal colorbar at the top\n",
    "    cbar_ax = fig.add_axes([0.25, 0.9, 0.5, 0.03])\n",
    "    cbar = fig.colorbar(sc1, cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "    # Set the color bar to only display min and max values\n",
    "    cbar.set_ticks([vmin, vmax])\n",
    "    cbar.set_ticklabels([f'{vmin}', f'{vmax}'])\n",
    "\n",
    "    # Remove the color bar box\n",
    "    cbar.outline.set_visible(False)\n",
    "    if modality == 'vision':\n",
    "        latex = r\"$r_{\\mathit{movie \\rightarrow story}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "\n",
    "        plt.savefig('results/movie_to_story/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    elif modality == 'language':\n",
    "        latex = r\"$r_{\\mathit{story \\rightarrow movie}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "        plt.savefig('results/story_to_movie/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
