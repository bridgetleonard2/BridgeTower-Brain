{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Full Protocol\n",
    "#### With GPU-accelerated Ridge Regression Using the Himalaya Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tutorial walks through the full crossmodal fMRI prediction process using the BridgeTower model. We will walk through extracting features from natural stimuli using BridgeTower layers, building voxelwise encoding models to predict fMRI data from stimuli features, and finally predicting language fMRI data using the vision encoding model and predicting visual fMRI data using the language encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters\n",
    "subject = 'S1'  # S1-S5\n",
    "modality = 'vision'  # vision or language\n",
    "layer = 8  # 1-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Feature Extraction\n",
    "We'll begin by putting our natural stimuli through the BridgeTower model and extracting feature representations from the layer specified above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Movie Stimuli\n",
    "Our movie data are stored in HDF format so we need a helper function to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5_array(file_name, key=None, slice=slice(0, None)):\n",
    "    \"\"\"Function to load data from an hdf file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name: string\n",
    "        hdf5 file name.\n",
    "    key: string\n",
    "        Key name to load. If not provided, all keys will be loaded.\n",
    "    slice: slice, or tuple of slices\n",
    "        Load only a slice of the hdf5 array. It will load `array[slice]`.\n",
    "        Use a tuple of slices to get a slice in multiple dimensions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array or dictionary\n",
    "        Array, or dictionary of arrays (if `key` is None).\n",
    "    \"\"\"\n",
    "    with h5py.File(file_name, mode='r') as hf:\n",
    "        if key is None:\n",
    "            data = dict()\n",
    "            for k in hf.keys():\n",
    "                data[k] = hf[k][slice]\n",
    "            return data\n",
    "        else:\n",
    "            return hf[key][slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/test.hdf', key='stimuli')\n",
    "train_00 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_00.hdf', key='stimuli')\n",
    "train_00 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_00.hdf', key='stimuli')\n",
    "train_01 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_01.hdf', key='stimuli')\n",
    "train_07 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_07.hdf', key='stimuli')\n",
    "train_03 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_03.hdf', key='stimuli')\n",
    "train_04 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_04.hdf', key='stimuli')\n",
    "train_05 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_05.hdf', key='stimuli')\n",
    "train_06 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_06.hdf', key='stimuli')\n",
    "train_07 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_07.hdf', key='stimuli')\n",
    "train_08 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_08.hdf', key='stimuli')\n",
    "train_09 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_09.hdf', key='stimuli')\n",
    "train_10 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_10.hdf', key='stimuli')\n",
    "train_11 = load_hdf5_array('data/raw_stimuli/shortclips/stimuli/train_11.hdf', key='stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Story Stimuli\n",
    "Our story transcripts are in TextGrid format so we want to load them into a list, we'll create a helper function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textgrid_to_array(textgrid):\n",
    "    \"\"\"Function to load transcript from textgrid into a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    textgrid: string\n",
    "        TextGrid file name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_transcript : Array\n",
    "        Array with each word in the story.\n",
    "    \"\"\"\n",
    "    if textgrid == 'data/raw_stimuli/textgrids/stimuli/legacy.TextGrid':\n",
    "        with open(textgrid, 'r')as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        full_transcript = []\n",
    "        # Important info starts at line 5\n",
    "        for line in data[5:]:\n",
    "            if line.startswith('2'):\n",
    "                index = data.index(line)\n",
    "                word = re.search(r'\"([^\"]*)\"', data[index+1].strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "    elif textgrid == 'data/raw_stimuli/textgrids/stimuli/life.TextGrid':\n",
    "        with open(textgrid, 'r') as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        full_transcript = []\n",
    "        for line in data:\n",
    "            if \"word\" in line:\n",
    "                index = data.index(line)\n",
    "                words = data[index+6:]  # this is where first word starts\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i % 3 == 0:\n",
    "                word = re.search(r'\"([^\"]*)\"', word.strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "    else:\n",
    "        with open(textgrid, 'r') as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        # Important info starts at line 8\n",
    "        for line in data[8:]:\n",
    "            # We only want item [2] info because those are the words instead\n",
    "            # of phonemes\n",
    "            if \"item [2]\" in line:\n",
    "                index = data.index(line)\n",
    "\n",
    "        summary_info = [line.strip() for line in data[index+1:index+6]]\n",
    "        print(summary_info)\n",
    "\n",
    "        word_script = data[index+6:]\n",
    "        full_transcript = []\n",
    "        for line in word_script:\n",
    "            if \"intervals\" in line:\n",
    "                # keep track of which interval we're on\n",
    "                ind = word_script.index(line)\n",
    "                word = re.search(r'\"([^\"]*)\"',\n",
    "                                 word_script[ind+3].strip()).group(1)\n",
    "                full_transcript.append(word)\n",
    "\n",
    "    return np.array(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternateithicatom = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/alternateithicatom.TextGrid\")\n",
    "avatar = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/avatar.TextGrid\")\n",
    "howtodraw = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/howtodraw.TextGrid\")\n",
    "legacy = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/legacy.TextGrid\")\n",
    "life = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/life.TextGrid\")\n",
    "myfirstdaywiththeyankees = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/myfirstdaywiththeyankees.TextGrid\")\n",
    "naked = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/naked.TextGrid\")\n",
    "odetostepfather = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/odetostepfather.TextGrid\")\n",
    "souls = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/souls.TextGrid\")\n",
    "undertheinfluence = textgrid_to_array(\"data/raw_stimuli/textgrids/stimuli/undertheinfluence.TextGrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run Stimuli through Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Helper Functions\n",
    "We need three functions, one to set-up the BridgeTower model, one to run the movie stimuli, and one to run the story stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(layer):\n",
    "    \"\"\"Function to setup transformers model with layer hooks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer: int\n",
    "        A layer reference for the BridgeTower model. Set's the forward\n",
    "        hook on the relevant layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    device : cuda or cpu for gpu acceleration if accessible.\n",
    "    model: BridgeTower model.\n",
    "    processor: BridgeTower processor.\n",
    "    features: Dictionary\n",
    "        A placeholder for batch features, one for each forward\n",
    "        hook.\n",
    "    layer_selected: Relevant layer chosen for forward hook.\n",
    "    \"\"\"\n",
    "    # Define Model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BridgeTowerModel.from_pretrained(\"BridgeTower/bridgetower-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define layers\n",
    "    model_layers = {\n",
    "            1: model.cross_modal_text_transform,\n",
    "            2: model.cross_modal_image_transform,\n",
    "            3: model.token_type_embeddings,\n",
    "            4: model.vision_model.visual.ln_post,\n",
    "            5: model.text_model.encoder.layer[-1].output.LayerNorm,\n",
    "            6: model.cross_modal_image_layers[-1].output,\n",
    "            7: model.cross_modal_text_layers[-1].output,\n",
    "            8: model.cross_modal_image_pooler,\n",
    "            9: model.cross_modal_text_pooler,\n",
    "            10: model.cross_modal_text_layernorm,\n",
    "            11: model.cross_modal_image_layernorm,\n",
    "            12: model.cross_modal_text_link_tower[-1],\n",
    "            13: model.cross_modal_image_link_tower[-1],\n",
    "        }\n",
    "\n",
    "    # placeholder for batch features\n",
    "    features = {}\n",
    "\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            # detached_outputs = [tensor.detach() for tensor in output]\n",
    "            last_output = output[-1].detach()\n",
    "            features[name] = last_output  # detached_outputs\n",
    "        return hook\n",
    "\n",
    "    # register forward hooks with layers of choice\n",
    "    layer_selected = model_layers[layer].register_forward_hook(\n",
    "        get_features(f'layer_{layer}'))\n",
    "\n",
    "    processor = BridgeTowerProcessor.from_pretrained(\n",
    "        \"BridgeTower/bridgetower-base\")\n",
    "\n",
    "    return device, model, processor, features, layer_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_features(movie_data, layer, n=30):\n",
    "    \"\"\"Function to average feature vectors over every n inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_data: Array\n",
    "        An array of shape (n_images, 512, 512). Represents frames from\n",
    "        a color movie.\n",
    "    n (optional): int\n",
    "        Number of frames to average over. Set at 30 to mimick an MRI\n",
    "        TR = 2 with a 15 fps movie.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Dictionary\n",
    "        Dictionary where keys are the model layer from which activations are\n",
    "        extracted. Values are lists representing activations of 768 dimensions\n",
    "        over the course of n_images / 30.\n",
    "    \"\"\"\n",
    "    print(\"loading HDF array\")\n",
    "    movie_data = load_hdf5_array(movie_data, key='stimuli')\n",
    "\n",
    "    print(\"Running movie through model\")\n",
    "\n",
    "    # Define Model\n",
    "    device, model, processor, features, layer_selected = setup_model(layer)\n",
    "\n",
    "    # create overall data structure for average feature vectors\n",
    "    # a dictionary with layer names as keys and a list of vectors as it values\n",
    "    data = {}\n",
    "\n",
    "    # a dictionary to store vectors for n consecutive trials\n",
    "    avg_data = {}\n",
    "\n",
    "    # loop through all inputs\n",
    "    for i, image in enumerate(movie_data):\n",
    "\n",
    "        model_input = processor(image, \"\", return_tensors=\"pt\")\n",
    "        # Assuming model_input is a dictionary of tensors\n",
    "        model_input = {key: value.to(device) for key,\n",
    "                       value in model_input.items()}\n",
    "\n",
    "        _ = model(**model_input)\n",
    "\n",
    "        for name, tensor in features.items():\n",
    "            if name not in avg_data:\n",
    "                avg_data[name] = []\n",
    "            avg_data[name].append(tensor)\n",
    "\n",
    "        # check if average should be stored\n",
    "        if (i + 1) % n == 0:\n",
    "            for name, tensors in avg_data.items():\n",
    "                first_size = tensors[0].size()\n",
    "\n",
    "                if all(tensor.size() == first_size for tensor in tensors):\n",
    "                    avg_feature = torch.mean(torch.stack(tensors), dim=0)\n",
    "                else:\n",
    "                    # Find problem dimension\n",
    "                    for dim in range(tensors[0].dim()):\n",
    "                        first_dim = tensors[0].size(dim)\n",
    "\n",
    "                        if not all(tensor.size(dim) == first_dim\n",
    "                                   for tensor in tensors):\n",
    "                            # Specify place to pad\n",
    "                            p_dim = (tensors[0].dim()*2) - (dim + 2)\n",
    "                            # print(p_dim)\n",
    "                            max_size = max(tensor.size(dim)\n",
    "                                           for tensor in tensors)\n",
    "                            padded_tensors = []\n",
    "\n",
    "                            for tensor in tensors:\n",
    "                                # Make a list with length of 2*dimensions - 1\n",
    "                                # to insert pad later\n",
    "                                pad_list = [0] * ((2*tensor[0].dim()) - 1)\n",
    "                                pad_list.insert(\n",
    "                                    p_dim, max_size - tensor.size(dim))\n",
    "                                # print(tuple(pad_list))\n",
    "                                padded_tensor = pad(tensor, tuple(pad_list))\n",
    "                                padded_tensors.append(padded_tensor)\n",
    "\n",
    "                    avg_feature = torch.mean(torch.stack(padded_tensors),\n",
    "                                             dim=0)\n",
    "\n",
    "                if name not in data:\n",
    "                    data[name] = []\n",
    "                data[name].append(avg_feature)\n",
    "\n",
    "            avg_data = {}\n",
    "\n",
    "    layer_selected.remove()\n",
    "\n",
    "    # Save data\n",
    "    data = data[f'layer_{layer}'].cpu()\n",
    "    data = data.numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_features(story_data, layer, n=20):\n",
    "    \"\"\"Function to extract feature vectors for each word of a story.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    story_data: Array\n",
    "        An array containing each word of the story in order.\n",
    "    n (optional): int\n",
    "        Number of words to to pad the target word with for\n",
    "        context (before and after).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : Dictionary\n",
    "        Dictionary where keys are the model layer from which activations are\n",
    "        extracted. Values are lists representing activations of 768 dimensions\n",
    "        over the course of each word in the story.\n",
    "    \"\"\"\n",
    "    print(\"loading textgrid\")\n",
    "    story_data = textgrid_to_array(story_data)\n",
    "\n",
    "    print(\"Running story through model\")\n",
    "    # Define Model\n",
    "    device, model, processor, features, layer_selected = setup_model(layer)\n",
    "\n",
    "    # Create a numpy array filled with gray values (128 in this case)\n",
    "    # THis will act as tthe zero image input***\n",
    "    gray_value = 128\n",
    "    image_array = np.full((512, 512, 3), gray_value, dtype=np.uint8)\n",
    "\n",
    "    # create overall data structure for average feature vectors\n",
    "    # a dictionary with layer names as keys and a list of vectors as it values\n",
    "    data = {}\n",
    "\n",
    "    # loop through all inputs\n",
    "    for i, word in enumerate(story_data):\n",
    "        # if one of first 20 words, just pad with all the words before it\n",
    "        if i < n:\n",
    "            # collapse list of strings into a single one\n",
    "            word_with_context = ' '.join(story_data[:(i+n)])\n",
    "        # if one of last 20 words, just pad with all the words after it\n",
    "        elif i > (len(story_data) - n):\n",
    "            # collapse list of strings into a single one\n",
    "            word_with_context = ' '.join(story_data[(i-n):])\n",
    "            # collapse list of strings into a single one\n",
    "        else:\n",
    "            word_with_context = ' '.join(story_data[(i-n):(i+n)])\n",
    "\n",
    "        model_input = processor(image_array, word_with_context,\n",
    "                                return_tensors=\"pt\")\n",
    "        # Assuming model_input is a dictionary of tensors\n",
    "        model_input = {key: value.to(device) for key,\n",
    "                       value in model_input.items()}\n",
    "\n",
    "        _ = model(**model_input)\n",
    "\n",
    "        for name, tensor in features.items():\n",
    "            if name not in data:\n",
    "                data[name] = []\n",
    "            data[name].append(tensor)\n",
    "\n",
    "    layer_selected.remove()\n",
    "\n",
    "    # Save data\n",
    "    data = data[f'layer_{layer}'].cpu()\n",
    "    data = data.numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Run Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = get_movie_features(test, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)\n",
    "train00_features = get_movie_features(train_00, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_features = get_story_features(alternateithicatom, layer)\n",
    "avatar_features = get_story_features(avatar, layer)\n",
    "howtodraw_features = get_story_features(howtodraw, layer)\n",
    "legacy_features = get_story_features(legacy, layer)\n",
    "life_features = get_story_features(life, layer)\n",
    "yankees_features = get_story_features(myfirstdaywiththeyankees, layer)\n",
    "naked_features = get_story_features(naked, layer)\n",
    "ode_features = get_story_features(odetostepfather, layer)\n",
    "souls_features = get_story_features(souls, layer)\n",
    "under_features = get_story_features(undertheinfluence, layer)\n",
    "souls_features = get_story_features(souls, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Voxelwise Encoding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatmap(subject, layer, correlations, modality):\n",
    "    \"\"\"Function to run the vision encoding model. Predicts brain activity\n",
    "    to story listening and return correlations between predictions and real\n",
    "    brain activity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject: string\n",
    "        A reference to the subject for analysis. Used to load fmri data.\n",
    "    layer: int\n",
    "        A layer reference for the BridgeTower model. Set's the forward\n",
    "        hook on the relevant layer.\n",
    "    correlations: array\n",
    "        Generated by story_prediction() or movie_prediction() function.\n",
    "        Contains the correlation between predicted and real brain activity\n",
    "        for each voxel.\n",
    "    modality: string\n",
    "        Which modality was used for the base encoding model: vision or\n",
    "        language.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Flatmaps:\n",
    "        Saves flatmap visualizations as pngs\n",
    "    \"\"\"\n",
    "    # Reverse flattening and masking\n",
    "    fmri_alternateithicatom = np.load(\"data/storydata/\" + subject +\n",
    "                                      \"/alternateithicatom.npy\")\n",
    "\n",
    "    mask = ~np.isnan(fmri_alternateithicatom[0])  # reference for the mask\n",
    "    # Initialize an empty 3D array with NaNs for the correlation data\n",
    "    reconstructed_correlations = np.full((31, 100, 100), np.nan)\n",
    "\n",
    "    # Flatten the mask to get the indices of the non-NaN data points\n",
    "    valid_indices = np.where(mask.flatten())[0]\n",
    "\n",
    "    # Assign the correlation coefficients to their original spatial positions\n",
    "    for index, corr_value in zip(valid_indices, correlations):\n",
    "        # Convert the 1D index back to 3D index in the spatial dimensions\n",
    "        z, x, y = np.unravel_index(index, (31, 100, 100))\n",
    "        reconstructed_correlations[z, x, y] = corr_value\n",
    "\n",
    "    flattened_correlations = reconstructed_correlations.flatten()\n",
    "\n",
    "    # Load mappers\n",
    "    lh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_lh.npz\")\n",
    "    lh_vertex_correlation_data = lh_mapping_matrix.dot(flattened_correlations)\n",
    "    lh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_lh.npy\")\n",
    "\n",
    "    rh_mapping_matrix = load_npz(\"data/mappers/\" + subject +\n",
    "                                 \"_listening_forVL_rh.npz\")\n",
    "    rh_vertex_correlation_data = rh_mapping_matrix.dot(flattened_correlations)\n",
    "    rh_vertex_coords = np.load(\"data/mappers/\" + subject +\n",
    "                               \"_vertex_coords_rh.npy\")\n",
    "\n",
    "    vmin, vmax = -0.1, 0.1\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4))\n",
    "\n",
    "    # Plot the first flatmap\n",
    "    sc1 = axs[0].scatter(lh_vertex_coords[:, 0], lh_vertex_coords[:, 1],\n",
    "                         c=lh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                         vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[0].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[0].set_title('Left Hemisphere')\n",
    "    axs[0].set_frame_on(False)\n",
    "    axs[0].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[0].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Plot the second flatmap\n",
    "    _ = axs[1].scatter(rh_vertex_coords[:, 0], rh_vertex_coords[:, 1],\n",
    "                       c=rh_vertex_correlation_data, cmap='RdBu_r',\n",
    "                       vmin=vmin, vmax=vmax, s=.005)\n",
    "    axs[1].set_aspect('equal', adjustable='box')  # Ensure equal scaling\n",
    "    # axs[1].set_title('Right Hemisphere')\n",
    "    axs[1].set_frame_on(False)\n",
    "    axs[1].set_xticks([])  # Remove x-axis ticks\n",
    "    axs[1].set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    # Adjust layout to make space for the top colorbar\n",
    "    plt.subplots_adjust(top=0.85, wspace=0)\n",
    "\n",
    "    # Add a single horizontal colorbar at the top\n",
    "    cbar_ax = fig.add_axes([0.25, 0.9, 0.5, 0.03])\n",
    "    cbar = fig.colorbar(sc1, cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "    # Set the color bar to only display min and max values\n",
    "    cbar.set_ticks([vmin, vmax])\n",
    "    cbar.set_ticklabels([f'{vmin}', f'{vmax}'])\n",
    "\n",
    "    # Remove the color bar box\n",
    "    cbar.outline.set_visible(False)\n",
    "    if modality == 'vision':\n",
    "        latex = r\"$r_{\\mathit{movie \\rightarrow story}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "\n",
    "        plt.savefig('results/movie_to_story/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    elif modality == 'language':\n",
    "        latex = r\"$r_{\\mathit{story \\rightarrow movie}}\"\n",
    "        plt.title(f'{subject}\\n{latex}$')\n",
    "        plt.savefig('results/story_to_movie/' + subject + '/layer' + layer +\n",
    "                    '_visual.png', format='png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
