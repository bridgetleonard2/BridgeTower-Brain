{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users/Bridget Leonard/Desktop/BridgeTower-Brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: fMRI Prediction\n",
    "- project features into opposite modal space with feature alignment\n",
    "- predict fMRI activity using features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Feature alignment matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the matrices\n",
    "These matrices were calculated in part 3. They represent the linear relationship between image and caption pairs. The image-->caption matrices were estimated by predicting each language feature from visual features, the caption--> image matrices were estimated by predicting each visual feature from the language features. Thus we have two matrices with size [768, 768] where each column represents a language or visual feature's linear relationship (beta coefficient) with each visual or language feature, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_image_matrices = np.load(\"results/feature_alignment/caption_to_image_matrices.npy\")\n",
    "image_to_caption_matrices = np.load(\"results/feature_alignment/image_to_caption_matrices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_to_image_matrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Project data into the opposite modal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature vectors\n",
    "# movie data\n",
    "test = np.load(\"results/feature_vectors/movie/test_data.npy\")\n",
    "train00 = np.load(\"results/feature_vectors/movie/train_00_data.npy\")\n",
    "train01 = np.load(\"results/feature_vectors/movie/train_01_data.npy\")\n",
    "train02 = np.load(\"results/feature_vectors/movie/train_02_data.npy\")\n",
    "train03 = np.load(\"results/feature_vectors/movie/train_03_data.npy\")\n",
    "train04 = np.load(\"results/feature_vectors/movie/train_04_data.npy\")\n",
    "train05 = np.load(\"results/feature_vectors/movie/train_05_data.npy\")\n",
    "train06 = np.load(\"results/feature_vectors/movie/train_06_data.npy\")\n",
    "train07 = np.load(\"results/feature_vectors/movie/train_07_data.npy\")\n",
    "train08 = np.load(\"results/feature_vectors/movie/train_08_data.npy\")\n",
    "train09 = np.load(\"results/feature_vectors/movie/train_09_data.npy\")\n",
    "train10 = np.load(\"results/feature_vectors/movie/train_10_data.npy\")\n",
    "train11 = np.load(\"results/feature_vectors/movie/train_11_data.npy\")\n",
    "\n",
    "# story data\n",
    "alternateithicatom = np.load(\"results/feature_vectors/story/alternateithicatom_data.npy\")\n",
    "avatar = np.load(\"results/feature_vectors/story/avatar_data.npy\")\n",
    "howtodraw = np.load(\"results/feature_vectors/story/howtodraw_data.npy\")\n",
    "legacy = np.load(\"results/feature_vectors/story/legacy_data.npy\")\n",
    "life = np.load(\"results/feature_vectors/story/life_data.npy\")\n",
    "myfirstdaywiththeyankees = np.load(\"results/feature_vectors/story/myfirstdaywiththeyankees_data.npy\")\n",
    "naked = np.load(\"results/feature_vectors/story/naked_data.npy\")\n",
    "odetostepfather = np.load(\"results/feature_vectors/story/odetostepfather_data.npy\")\n",
    "souls = np.load(\"results/feature_vectors/story/souls_data.npy\")\n",
    "undertheinfluence = np.load(\"results/feature_vectors/story/undertheinfluence_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual to text: Movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed = np.dot(test, image_to_caption_matrices.T)\n",
    "train00_transformed = np.dot(train00, image_to_caption_matrices.T)\n",
    "train01_transformed = np.dot(train01, image_to_caption_matrices.T)\n",
    "train02_transformed = np.dot(train02, image_to_caption_matrices.T)\n",
    "train03_transformed = np.dot(train03, image_to_caption_matrices.T)\n",
    "train04_transformed = np.dot(train04, image_to_caption_matrices.T)\n",
    "train05_transformed = np.dot(train05, image_to_caption_matrices.T)\n",
    "train06_transformed = np.dot(train06, image_to_caption_matrices.T)\n",
    "train07_transformed = np.dot(train07, image_to_caption_matrices.T)\n",
    "train08_transformed = np.dot(train08, image_to_caption_matrices.T)\n",
    "train09_transformed = np.dot(train09, image_to_caption_matrices.T)\n",
    "train10_transformed = np.dot(train10, image_to_caption_matrices.T)\n",
    "train11_transformed = np.dot(train11, image_to_caption_matrices.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to visual: Story data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternateithicatom_transformed = np.dot(alternateithicatom, caption_to_image_matrices.T)\n",
    "avatar_transformed = np.dot(avatar, caption_to_image_matrices.T)\n",
    "howtodraw_transformed = np.dot(howtodraw, caption_to_image_matrices.T)\n",
    "legacy_transformed = np.dot(legacy, caption_to_image_matrices.T)\n",
    "life_transformed = np.dot(life, caption_to_image_matrices.T)\n",
    "myfirstdaywiththeyankees_transformed = np.dot(myfirstdaywiththeyankees, caption_to_image_matrices.T)\n",
    "naked_transformed = np.dot(naked, caption_to_image_matrices.T)\n",
    "odetostepfather_transformed = np.dot(odetostepfather, caption_to_image_matrices.T)\n",
    "souls_transformed = np.dot(souls, caption_to_image_matrices.T)\n",
    "undertheinfluence_transformed = np.dot(undertheinfluence, caption_to_image_matrices.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Load voxelwise encoding models\n",
    "These matrices were calculated in part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Bridget Leonard\\\\Desktop\\\\BridgeTower-Brain'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the starting voxel number from the file name\n",
    "def extract_start_number(filename):\n",
    "    match = re.search(r'(\\d+)-\\d+\\.npy$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    # Handle files with underscores\n",
    "    match = re.search(r'(\\d+)_\\d+\\.npy$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the .npy files\n",
    "directory_path = 'results/encoding_model/movie/coefficients'\n",
    "\n",
    "# List all .npy files and sort them based on the starting voxel number\n",
    "npy_files = [f for f in os.listdir(directory_path) if f.endswith('.npy')]\n",
    "npy_files.sort(key=extract_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coefficients_0_1599.npy',\n",
       " 'coefficients_1600_3199.npy',\n",
       " 'coefficients_3200_4799.npy',\n",
       " 'coefficients_4800_6399.npy',\n",
       " 'coefficients_6400_7999.npy',\n",
       " 'coefficients_8000_9599.npy',\n",
       " 'coefficients_9600_11199.npy',\n",
       " 'coefficients_11200_12799.npy',\n",
       " 'coefficients_12800_14399.npy',\n",
       " 'coefficients_14400_15999.npy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the data arrays\n",
    "data_arrays = []\n",
    "\n",
    "for file_name in npy_files:\n",
    "    # Load the current batch and transpose it\n",
    "    batch_data = np.load(os.path.join(directory_path, file_name)).T  # Transpose here\n",
    "    data_arrays.append(batch_data)\n",
    "\n",
    "# Combine all the transposed batches into one final matrix\n",
    "# Note: np.concatenate operates along the first axis by default, so this aligns with our goal\n",
    "final_matrix = np.concatenate(data_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coefficients_0_1599.npy',\n",
       " 'coefficients_1600_3199.npy',\n",
       " 'coefficients_3200_4799.npy',\n",
       " 'coefficients_4800_6399.npy',\n",
       " 'coefficients_6400_7999.npy',\n",
       " 'coefficients_8000_9599.npy',\n",
       " 'coefficients_9600_11199.npy',\n",
       " 'coefficients_11200_12799.npy',\n",
       " 'coefficients_12800_14399.npy',\n",
       " 'coefficients_14400_15999.npy',\n",
       " 'coefficients_16000_17599.npy',\n",
       " 'coefficients_17600_19199.npy',\n",
       " 'coefficients_19200_20799.npy',\n",
       " 'coefficients_20800_22399.npy',\n",
       " 'coefficients_22400_23999.npy',\n",
       " 'coefficients_24000_25599.npy',\n",
       " 'coefficients_25600_27199.npy',\n",
       " 'coefficients_27200_28799.npy',\n",
       " 'coefficients_28800_30399.npy',\n",
       " 'coefficients_30400_31999.npy',\n",
       " 'coefficients_32000_33599.npy',\n",
       " 'coefficients_33600_35199.npy',\n",
       " 'coefficients_35200_36799.npy',\n",
       " 'coefficients_36800_38399.npy',\n",
       " 'coefficients_38400_39999.npy',\n",
       " 'coefficients_40000_41599.npy',\n",
       " 'coefficients_41600_43199.npy',\n",
       " 'coefficients_43200_44799.npy',\n",
       " 'coefficients_44800_46399.npy',\n",
       " 'coefficients_46400_47999.npy',\n",
       " 'coefficients_48000_49599.npy',\n",
       " 'coefficients_49600_51199.npy',\n",
       " 'coefficients_51200_52799.npy',\n",
       " 'coefficients_52800_54399.npy',\n",
       " 'coefficients_54400_55999.npy',\n",
       " 'coefficients_56000_57599.npy',\n",
       " 'coefficients_57600_59199.npy',\n",
       " 'coefficients_59200_60799.npy',\n",
       " 'coefficients_60800_62399.npy',\n",
       " 'coefficients_62400_63999.npy',\n",
       " 'coefficients_64000_65599.npy',\n",
       " 'coefficients_65600_67199.npy',\n",
       " 'coefficients_67200_68799.npy',\n",
       " 'coefficients_68800_70399.npy',\n",
       " 'coefficients_70400_71999.npy',\n",
       " 'coefficients_72000_73599.npy',\n",
       " 'coefficients_73600_75199.npy',\n",
       " 'coefficients_75200_76799.npy',\n",
       " 'coefficients_76800_78399.npy',\n",
       " 'coefficients_78400_79999.npy',\n",
       " 'coefficients_80000-81111.npy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_for_voxel(voxel_number):\n",
    "    for filename in npy_files:\n",
    "        # Extract the start and end numbers from the filename\n",
    "        match = re.search(r'(\\d+)_(\\d+)\\.npy', filename)\n",
    "        if match:\n",
    "            start_num, end_num = int(match.group(1)), int(match.group(2))\n",
    "            # Check if the voxel number falls within the range\n",
    "            if start_num <= voxel_number <= end_num:\n",
    "                # Calculate the index of the voxel within the file\n",
    "                voxel_index_within_file = voxel_number - start_num\n",
    "                return filename, voxel_index_within_file\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final_matrix(voxel):\n",
    "    file_name, voxel_index = find_file_for_voxel(voxel)\n",
    "    file = np.load(os.path.join(directory_path, file_name))\n",
    "    assert np.array_equal(file[voxel_index], final_matrix[:, voxel]), \"Voxel coefficients do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_matrix(0)\n",
    "test_final_matrix(47800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81600)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 81111)\n"
     ]
    }
   ],
   "source": [
    "# Find columns that are not all zeroes\n",
    "not_all_zeroes = np.any(final_matrix != 0, axis=0)\n",
    "\n",
    "# Filter out columns that are all zeroes\n",
    "final_matrix = final_matrix[:, not_all_zeroes]\n",
    "\n",
    "print(final_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Using the vision encoding model to predict fMRI responses to stories\n",
    "We'll be using the tranformed story data above to predict the fMRI data found in `data/fmri_data/storydata/S1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_alternateithicatom = np.load(\"data/fmri_data/storydata/S1/alternateithicatom.npy\")\n",
    "s1_avatar = np.load(\"data/fmri_data/storydata/S1/avatar.npy\")\n",
    "s1_howtodraw = np.load(\"data/fmri_data/storydata/S1/howtodraw.npy\")\n",
    "s1_legacy = np.load(\"data/fmri_data/storydata/S1/legacy.npy\")\n",
    "s1_life = np.load(\"data/fmri_data/storydata/S1/life.npy\")\n",
    "s1_myfirstdaywiththeyankees = np.load(\"data/fmri_data/storydata/S1/myfirstdaywiththeyankees.npy\")\n",
    "s1_naked = np.load(\"data/fmri_data/storydata/S1/naked.npy\")\n",
    "s1_odetostepfather = np.load(\"data/fmri_data/storydata/S1/odetostepfather.npy\")\n",
    "s1_souls = np.load(\"data/fmri_data/storydata/S1/souls.npy\")\n",
    "s1_undertheinfluence = np.load(\"data/fmri_data/storydata/S1/undertheinfluence.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with first story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 31, 100, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_alternateithicatom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(data):\n",
    "    mask = ~np.isnan(data)\n",
    "\n",
    "    # Apply the mask and then flatten\n",
    "    # This will keep only the non-NaN values\n",
    "    data_reshaped = data[mask].reshape(data.shape[0], -1)\n",
    "    \n",
    "    print(\"fMRI shape:\", data_reshaped.shape)\n",
    "    return data_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI shape: (363, 81111)\n"
     ]
    }
   ],
   "source": [
    "s1_ai_reshaped = remove_nan(s1_alternateithicatom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to fMRI acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2681, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternateithicatom_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_acq(feature_data, fmri_data):\n",
    "    dimensions = fmri_data.shape[0]\n",
    "    data_transposed = feature_data.T\n",
    "    data_resampled = np.empty((data_transposed.shape[0], dimensions))\n",
    "\n",
    "    for i in range(data_transposed.shape[0]):\n",
    "        data_resampled[i, :] = resample(data_transposed[i, :], dimensions, window=('kaiser', 14))\n",
    "    \n",
    "    print(\"Shape after resampling:\", data_resampled.T.shape)\n",
    "    return data_resampled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after resampling: (363, 768)\n"
     ]
    }
   ],
   "source": [
    "alternateithicatom_resampled = resample_to_acq(alternateithicatom_transformed, s1_ai_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delay the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_features(features):\n",
    "    delays = [2, 4, 6, 8]  # Delays in seconds\n",
    "    shifted_features_list = []\n",
    "\n",
    "    for delay in delays:\n",
    "        shift_amount = delay // 2  # Assuming TR is 2 seconds\n",
    "        shifted = np.roll(features, shift_amount, axis=0)\n",
    "        # Optionally, handle edge effects here (e.g., zero-padding or trimming)\n",
    "        shifted_features_list.append(shifted)\n",
    "\n",
    "    # Stack the shifted arrays to create a 3D array\n",
    "    shifted_features_3d = np.stack(shifted_features_list, axis=-1)\n",
    "    \n",
    "    # Reshape the feature data for regression\n",
    "    n_time_points, n_features, n_delays = shifted_features_3d.shape\n",
    "    features_reshaped = shifted_features_3d.reshape(n_time_points, n_features * n_delays)\n",
    "\n",
    "    print(\"Shape after delays:\", features_reshaped.shape)\n",
    "    return features_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after delays: (363, 3072)\n"
     ]
    }
   ],
   "source": [
    "ai_resamp_delay = delay_features(alternateithicatom_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create overall function to prepare feature and fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(fmri_data, feature_data):\n",
    "    fmri_reshaped = remove_nan(fmri_data)\n",
    "\n",
    "    feature_resampled = resample_to_acq(feature_data, fmri_reshaped)\n",
    "    feature_delayed = delay_features(feature_resampled)\n",
    "\n",
    "    return fmri_reshaped, feature_delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fMRI predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI shape: (363, 81111)\n",
      "Shape after resampling: (363, 768)\n",
      "Shape after delays: (363, 3072)\n"
     ]
    }
   ],
   "source": [
    "s1_ai_fmri, s1_ai_features = prep_data(s1_alternateithicatom, alternateithicatom_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_ai_predictions = np.dot(s1_ai_features, final_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_correlation(x, y):\n",
    "    \"\"\"Calculate the Pearson correlation coefficient safely.\"\"\"\n",
    "    # Mean centering\n",
    "    x_mean = x - np.mean(x)\n",
    "    y_mean = y - np.mean(y)\n",
    "    \n",
    "    # Numerator: sum of the product of mean-centered variables\n",
    "    numerator = np.sum(x_mean * y_mean)\n",
    "    \n",
    "    # Denominator: sqrt of the product of the sums of squared mean-centered variables\n",
    "    denominator = np.sqrt(np.sum(x_mean**2) * np.sum(y_mean**2))\n",
    "    \n",
    "    # Safe division\n",
    "    if denominator == 0:\n",
    "        # Return NaN or another value to indicate undefined correlation\n",
    "        return np.nan\n",
    "    else:\n",
    "        return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation(predicted_fMRI, real_fMRI):\n",
    "    # Calculate correlations for each voxel\n",
    "    correlation_coefficients = np.array([safe_correlation(predicted_fMRI[:, i], real_fMRI[:, i]) for i in range(predicted_fMRI.shape[1])])\n",
    "\n",
    "    # Check for NaNs in the result to identify voxels with undefined correlations\n",
    "    nans_in_correlations = np.isnan(correlation_coefficients).any()\n",
    "    print(f\"NaNs in correlation coefficients: {nans_in_correlations}\")\n",
    "\n",
    "    return correlation_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in correlation coefficients: True\n"
     ]
    }
   ],
   "source": [
    "s1_ai_correlations = calc_correlation(s1_ai_predictions, s1_ai_fmri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a function to run through full correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_corr(fmri_data, feature_data, encoding_matrix):\n",
    "    fmri_reshaped, feature_delayed = prep_data(fmri_data, feature_data)\n",
    "    predicted_fmri = np.dot(feature_delayed, encoding_matrix)\n",
    "    correlation = calc_correlation(predicted_fmri, fmri_reshaped)\n",
    "    print(np.nanmax(correlation))\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI shape: (363, 81111)\n",
      "Shape after resampling: (363, 768)\n",
      "Shape after delays: (363, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.36196507671486794\n"
     ]
    }
   ],
   "source": [
    "s1_ai_correlations = predict_and_corr(s1_alternateithicatom, alternateithicatom_transformed, final_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI shape: (387, 81111)\n",
      "Shape after resampling: (387, 768)\n",
      "Shape after delays: (387, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.40304479820845945\n",
      "fMRI shape: (374, 81111)\n",
      "Shape after resampling: (374, 768)\n",
      "Shape after delays: (374, 3072)\n",
      "NaNs in correlation coefficients: False\n",
      "0.4150796918910574\n",
      "fMRI shape: (420, 81111)\n",
      "Shape after resampling: (420, 768)\n",
      "Shape after delays: (420, 3072)\n",
      "NaNs in correlation coefficients: False\n",
      "0.3398571936128742\n",
      "fMRI shape: (450, 81111)\n",
      "Shape after resampling: (450, 768)\n",
      "Shape after delays: (450, 3072)\n",
      "NaNs in correlation coefficients: False\n",
      "0.30989681940801767\n",
      "fMRI shape: (378, 81111)\n",
      "Shape after resampling: (378, 768)\n",
      "Shape after delays: (378, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.33527138585387634\n",
      "fMRI shape: (442, 81111)\n",
      "Shape after resampling: (442, 768)\n",
      "Shape after delays: (442, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.32265857428990086\n",
      "fMRI shape: (424, 81111)\n",
      "Shape after resampling: (424, 768)\n",
      "Shape after delays: (424, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.3438225635765908\n",
      "fMRI shape: (375, 81111)\n",
      "Shape after resampling: (375, 768)\n",
      "Shape after delays: (375, 3072)\n",
      "NaNs in correlation coefficients: True\n",
      "0.3793556247516554\n",
      "fMRI shape: (324, 81111)\n",
      "Shape after resampling: (324, 768)\n",
      "Shape after delays: (324, 3072)\n",
      "NaNs in correlation coefficients: False\n",
      "0.48976981742327286\n"
     ]
    }
   ],
   "source": [
    "s1_avatar_correlations = predict_and_corr(s1_avatar, avatar_transformed, final_matrix)\n",
    "s1_howtodraw_correlations = predict_and_corr(s1_howtodraw, howtodraw_transformed, final_matrix)\n",
    "s1_legacy_correlations = predict_and_corr(s1_legacy, legacy_transformed, final_matrix)\n",
    "s1_life_correlations = predict_and_corr(s1_life, life_transformed, final_matrix)\n",
    "s1_yankees_correlations = predict_and_corr(s1_myfirstdaywiththeyankees, myfirstdaywiththeyankees_transformed, final_matrix)\n",
    "s1_naked_correlations = predict_and_corr(s1_naked, naked_transformed, final_matrix)\n",
    "s1_ode_correlations = predict_and_corr(s1_odetostepfather, odetostepfather_transformed, final_matrix)\n",
    "s1_souls_correlations = predict_and_corr(s1_souls, souls_transformed, final_matrix)\n",
    "s1_under_correlations = predict_and_corr(s1_undertheinfluence, undertheinfluence_transformed, final_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correlations = np.stack((s1_ai_correlations, s1_avatar_correlations, s1_howtodraw_correlations, s1_legacy_correlations, s1_life_correlations,\n",
    "                             s1_yankees_correlations, s1_naked_correlations, s1_ode_correlations, s1_souls_correlations, s1_under_correlations))\n",
    "average_correlation = np.nanmean(all_correlations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01195322, -0.02056186,  0.02949554,  0.03822953,  0.03202731,\n",
       "        0.02950777,  0.02026067,  0.03076557, -0.00979232, -0.00758311])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_correlation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81111,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_correlation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse flattening and masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 31, 100, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_alternateithicatom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 100, 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the mask used for flattening (assuming you have access to 's1_movie_train' or similar)\n",
    "mask = ~np.isnan(s1_alternateithicatom[0])  # Using the first time point as a reference for the mask\n",
    "\n",
    "# Initialize an empty 3D array with NaNs for the correlation data\n",
    "reconstructed_correlations = np.full((31, 100, 100), np.nan)\n",
    "\n",
    "# Flatten the mask to get the indices of the original valid (non-NaN) data points\n",
    "valid_indices = np.where(mask.flatten())[0]\n",
    "\n",
    "# Assign the correlation coefficients to their original spatial positions\n",
    "for index, corr_value in zip(valid_indices, average_correlation):\n",
    "    # Convert the 1D index back to 3D index in the spatial dimensions\n",
    "    z, x, y = np.unravel_index(index, (31, 100, 100))\n",
    "    reconstructed_correlations[z, x, y] = corr_value\n",
    "\n",
    "reconstructed_correlations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nilearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plotting\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nilearn'"
     ]
    }
   ],
   "source": [
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
