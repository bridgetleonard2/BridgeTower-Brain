{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users/Bridget Leonard/Desktop/results-Brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: fMRI Prediction\n",
    "- project features into opposite modal space with feature alignment\n",
    "- predict fMRI activity using features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Feature alignment matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the matrices\n",
    "These matrices were calculated in part 3. They represent the linear relationship between image and caption pairs. The image-->caption matrices were estimated by predicting each language feature from visual features, the caption--> image matrices were estimated by predicting each visual feature from the language features. Thus we have two matrices with size [768, 768] where each column represents a language or visual feature's linear relationship (beta coefficient) with each visual or language feature, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_image_matrices = np.load(\"results/feature_alignment/caption_to_image_matrices.npy\")\n",
    "image_to_caption_matrices = np.load(\"results/feature_alignment/image_to_caption_matrices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_to_image_matrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Project data into the opposite modal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature vectors\n",
    "# movie data\n",
    "test = np.load(\"results/feature_vectors/movie/test_data.npy\")\n",
    "train00 = np.load(\"results/feature_vectors/movie/train_00_data.npy\")\n",
    "train01 = np.load(\"results/feature_vectors/movie/train_01_data.npy\")\n",
    "train02 = np.load(\"results/feature_vectors/movie/train_02_data.npy\")\n",
    "train03 = np.load(\"results/feature_vectors/movie/train_03_data.npy\")\n",
    "train04 = np.load(\"results/feature_vectors/movie/train_04_data.npy\")\n",
    "train05 = np.load(\"results/feature_vectors/movie/train_05_data.npy\")\n",
    "train06 = np.load(\"results/feature_vectors/movie/train_06_data.npy\")\n",
    "train07 = np.load(\"results/feature_vectors/movie/train_07_data.npy\")\n",
    "train08 = np.load(\"results/feature_vectors/movie/train_08_data.npy\")\n",
    "train09 = np.load(\"results/feature_vectors/movie/train_09_data.npy\")\n",
    "train10 = np.load(\"results/feature_vectors/movie/train_10_data.npy\")\n",
    "train11 = np.load(\"results/feature_vectors/movie/train_11_data.npy\")\n",
    "\n",
    "# story data\n",
    "alternateithicatom = np.load(\"results/feature_vectors/story/alternateithicatom_data.npy\")\n",
    "avatar = np.load(\"results/feature_vectors/story/avatar_data.npy\")\n",
    "howtodraw = np.load(\"results/feature_vectors/story/howtodraw_data.npy\")\n",
    "legacy = np.load(\"results/feature_vectors/story/legacy_data.npy\")\n",
    "life = np.load(\"results/feature_vectors/story/life_data.npy\")\n",
    "myfirstdaywiththeyankees = np.load(\"results/feature_vectors/story/myfirstdaywiththeyankees_data.npy\")\n",
    "naked = np.load(\"results/feature_vectors/story/naked_data.npy\")\n",
    "odetostepfather = np.load(\"results/feature_vectors/story/odetostepfather_data.npy\")\n",
    "souls = np.load(\"results/feature_vectors/story/souls_data.npy\")\n",
    "undertheinfluence = np.load(\"results/feature_vectors/story/undertheinfluence_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual to text: Movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed = np.dot(test, image_to_caption_matrices.T)\n",
    "train00_transformed = np.dot(train00, image_to_caption_matrices.T)\n",
    "train01_transformed = np.dot(train01, image_to_caption_matrices.T)\n",
    "train02_transformed = np.dot(train02, image_to_caption_matrices.T)\n",
    "train03_transformed = np.dot(train03, image_to_caption_matrices.T)\n",
    "train04_transformed = np.dot(train04, image_to_caption_matrices.T)\n",
    "train05_transformed = np.dot(train05, image_to_caption_matrices.T)\n",
    "train06_transformed = np.dot(train06, image_to_caption_matrices.T)\n",
    "train07_transformed = np.dot(train07, image_to_caption_matrices.T)\n",
    "train08_transformed = np.dot(train08, image_to_caption_matrices.T)\n",
    "train09_transformed = np.dot(train09, image_to_caption_matrices.T)\n",
    "train10_transformed = np.dot(train10, image_to_caption_matrices.T)\n",
    "train11_transformed = np.dot(train11, image_to_caption_matrices.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to visual: Story data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternateithicatom_transformed = np.dot(alternateithicatom, caption_to_image_matrices.T)\n",
    "avatar_transformed = np.dot(avatar, caption_to_image_matrices.T)\n",
    "howtodraw_transformed = np.dot(howtodraw, caption_to_image_matrices.T)\n",
    "legacy_transformed = np.dot(legacy, caption_to_image_matrices.T)\n",
    "life_transformed = np.dot(life, caption_to_image_matrices.T)\n",
    "myfirstdaywiththeyankees_transformed = np.dot(myfirstdaywiththeyankees, caption_to_image_matrices.T)\n",
    "naked_transformed = np.dot(naked, caption_to_image_matrices.T)\n",
    "odetostepfather_transformed = np.dot(odetostepfather, caption_to_image_matrices.T)\n",
    "souls_transformed = np.dot(souls, caption_to_image_matrices.T)\n",
    "undertheinfluence_transformed = np.dot(undertheinfluence, caption_to_image_matrices.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Load voxelwise encoding models\n",
    "These matrices were calculated in part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Bridget Leonard\\\\Desktop\\\\BridgeTower-Brain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the starting voxel number from the file name\n",
    "def extract_start_number(filename):\n",
    "    match = re.search(r'(\\d+)-\\d+\\.npy$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    # Handle files with underscores\n",
    "    match = re.search(r'(\\d+)_\\d+\\.npy$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the .npy files\n",
    "directory_path = 'results/encoding_model/movie/coefficients'\n",
    "\n",
    "# List all .npy files and sort them based on the starting voxel number\n",
    "npy_files = [f for f in os.listdir(directory_path) if f.endswith('.npy')]\n",
    "npy_files.sort(key=extract_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coefficients_0_1599.npy',\n",
       " 'coefficients_1600_3199.npy',\n",
       " 'coefficients_3200_4799.npy',\n",
       " 'coefficients_4800_6399.npy',\n",
       " 'coefficients_6400_7999.npy',\n",
       " 'coefficients_8000_9599.npy',\n",
       " 'coefficients_9600_11199.npy',\n",
       " 'coefficients_11200_12799.npy',\n",
       " 'coefficients_12800_14399.npy',\n",
       " 'coefficients_14400_15999.npy']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the data arrays\n",
    "data_arrays = []\n",
    "\n",
    "for file_name in npy_files:\n",
    "    # Load the current batch and transpose it\n",
    "    batch_data = np.load(os.path.join(directory_path, file_name)).T  # Transpose here\n",
    "    data_arrays.append(batch_data)\n",
    "\n",
    "# Combine all the transposed batches into one final matrix\n",
    "# Note: np.concatenate operates along the first axis by default, so this aligns with our goal\n",
    "final_matrix = np.concatenate(data_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coefficients_0_1599.npy',\n",
       " 'coefficients_1600_3199.npy',\n",
       " 'coefficients_3200_4799.npy',\n",
       " 'coefficients_4800_6399.npy',\n",
       " 'coefficients_6400_7999.npy',\n",
       " 'coefficients_8000_9599.npy',\n",
       " 'coefficients_9600_11199.npy',\n",
       " 'coefficients_11200_12799.npy',\n",
       " 'coefficients_12800_14399.npy',\n",
       " 'coefficients_14400_15999.npy',\n",
       " 'coefficients_16000_17599.npy',\n",
       " 'coefficients_17600_19199.npy',\n",
       " 'coefficients_19200_20799.npy',\n",
       " 'coefficients_20800_22399.npy',\n",
       " 'coefficients_22400_23999.npy',\n",
       " 'coefficients_24000_25599.npy',\n",
       " 'coefficients_25600_27199.npy',\n",
       " 'coefficients_27200_28799.npy',\n",
       " 'coefficients_28800_30399.npy',\n",
       " 'coefficients_30400_31999.npy',\n",
       " 'coefficients_32000_33599.npy',\n",
       " 'coefficients_33600_35199.npy',\n",
       " 'coefficients_35200_36799.npy',\n",
       " 'coefficients_36800_38399.npy',\n",
       " 'coefficients_38400_39999.npy',\n",
       " 'coefficients_40000_41599.npy',\n",
       " 'coefficients_41600_43199.npy',\n",
       " 'coefficients_43200_44799.npy',\n",
       " 'coefficients_44800_46399.npy',\n",
       " 'coefficients_46400_47999.npy',\n",
       " 'coefficients_48000_49599.npy',\n",
       " 'coefficients_49600_51199.npy',\n",
       " 'coefficients_51200_52799.npy',\n",
       " 'coefficients_52800_54399.npy',\n",
       " 'coefficients_54400_55999.npy',\n",
       " 'coefficients_56000_57599.npy',\n",
       " 'coefficients_57600_59199.npy',\n",
       " 'coefficients_59200_60799.npy',\n",
       " 'coefficients_60800_62399.npy',\n",
       " 'coefficients_62400_63999.npy',\n",
       " 'coefficients_64000_65599.npy',\n",
       " 'coefficients_65600_67199.npy',\n",
       " 'coefficients_67200_68799.npy',\n",
       " 'coefficients_68800_70399.npy',\n",
       " 'coefficients_70400_71999.npy',\n",
       " 'coefficients_72000_73599.npy',\n",
       " 'coefficients_73600_75199.npy',\n",
       " 'coefficients_75200_76799.npy',\n",
       " 'coefficients_76800_78399.npy',\n",
       " 'coefficients_78400_79999.npy',\n",
       " 'coefficients_80000-81111.npy']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_for_voxel(voxel_number):\n",
    "    for filename in npy_files:\n",
    "        # Extract the start and end numbers from the filename\n",
    "        match = re.search(r'(\\d+)_(\\d+)\\.npy', filename)\n",
    "        if match:\n",
    "            start_num, end_num = int(match.group(1)), int(match.group(2))\n",
    "            # Check if the voxel number falls within the range\n",
    "            if start_num <= voxel_number <= end_num:\n",
    "                # Calculate the index of the voxel within the file\n",
    "                voxel_index_within_file = voxel_number - start_num\n",
    "                return filename, voxel_index_within_file\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final_matrix(voxel):\n",
    "    file_name, voxel_index = find_file_for_voxel(voxel)\n",
    "    file = np.load(os.path.join(directory_path, file_name))\n",
    "    assert np.array_equal(file[voxel_index], final_matrix[:, voxel]), \"Voxel coefficients do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_matrix(0)\n",
    "test_final_matrix(47800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 81600)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 81111)\n"
     ]
    }
   ],
   "source": [
    "# Find columns that are not all zeroes\n",
    "not_all_zeroes = np.any(final_matrix != 0, axis=0)\n",
    "\n",
    "# Filter out columns that are all zeroes\n",
    "final_matrix = final_matrix[:, not_all_zeroes]\n",
    "\n",
    "print(final_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the vision encoding model to predict fMRI responses to stories\n",
    "We'll be using the tranformed story data above to predict the fMRI data found in `data/fmri_data/storydata/S1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_alternateithicatom = np.load(\"data/fmri_data/storydata/S1/alternateithicatom.npy\")\n",
    "s1_avatar = np.load(\"data/fmri_data/storydata/S1/avatar.npy\")\n",
    "s1_howtodraw = np.load(\"data/fmri_data/storydata/S1/howtodraw.npy\")\n",
    "s1_legacy = np.load(\"data/fmri_data/storydata/S1/legacy.npy\")\n",
    "s1_life = np.load(\"data/fmri_data/storydata/S1/life.npy\")\n",
    "s1_myfirstdaywiththeyankees = np.load(\"data/fmri_data/storydata/S1/myfirstdaywiththeyankees.npy\")\n",
    "s1_naked = np.load(\"data/fmri_data/storydata/S1/naked.npy\")\n",
    "s1_odetostepfather = np.load(\"data/fmri_data/storydata/S1/odetostepfather.npy\")\n",
    "s1_souls = np.load(\"data/fmri_data/storydata/S1/souls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 31, 100, 100)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_alternateithicatom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(s1_alternateithicatom)\n",
    "\n",
    "# Apply the mask and then flatten\n",
    "# This will keep only the non-NaN values\n",
    "s1_alternateithicatom_reshaped = s1_alternateithicatom[mask].reshape(s1_alternateithicatom.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 81111)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_alternateithicatom_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to fMRI acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2681, 768)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternateithicatom_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_acq(feature_data, fmri_data):\n",
    "    dimensions = fmri_data.shape[0]\n",
    "    data_transposed = feature_data.T\n",
    "    data_resampled = np.empty((data_transposed.shape[0], dimensions))\n",
    "\n",
    "    for i in range(data_transposed.shape[0]):\n",
    "        data_resampled[i, :] = resample(data_transposed[i, :], 363, window=('kaiser', 14))\n",
    "    \n",
    "    print(\"Final shape:\", data_resampled.T.shape)\n",
    "    return data_resampled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (363, 768)\n"
     ]
    }
   ],
   "source": [
    "alternateithicatom_resampled = resample_to_acq(alternateithicatom_transformed, s1_alternateithicatom_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delay the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_features(features):\n",
    "    delays = [2, 4, 6, 8]  # Delays in seconds\n",
    "    shifted_features_list = []\n",
    "\n",
    "    for delay in delays:\n",
    "        shift_amount = delay // 2  # Assuming TR is 2 seconds\n",
    "        shifted = np.roll(features, shift_amount, axis=0)\n",
    "        # Optionally, handle edge effects here (e.g., zero-padding or trimming)\n",
    "        shifted_features_list.append(shifted)\n",
    "\n",
    "    # Stack the shifted arrays to create a 3D array\n",
    "    shifted_features_3d = np.stack(shifted_features_list, axis=-1)\n",
    "    \n",
    "    # Reshape the feature data for regression\n",
    "    n_time_points, n_features, n_delays = shifted_features_3d.shape\n",
    "    features_reshaped = shifted_features_3d.reshape(n_time_points, n_features * n_delays)\n",
    "\n",
    "    return features_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 3072)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_resamp_delay = delay_features(alternateithicatom_resampled)\n",
    "ai_resamp_delay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
